{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/jasper/Desktop/Introduction_AIML/venv/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/jasper/Desktop/Introduction_AIML/venv/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/jasper/Desktop/Introduction_AIML/venv/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/jasper/Desktop/Introduction_AIML/venv/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/jasper/Desktop/Introduction_AIML/venv/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/jasper/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['misc', 'tokenizers', '.DS_Store', 'stemmers', 'chunkers', 'models', 'nltk_data-gh-pages', 'grammars', 'taggers', 'sentiment', 'corpora', 'help']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "# Set the NLTK_DATA environment variable\n",
    "os.environ['NLTK_DATA'] = '/Users/jasper/nltk_data'\n",
    "\n",
    "# Append the NLTK data path\n",
    "nltk.data.path.append('/Users/jasper/nltk_data')\n",
    "\n",
    "# List the contents of the NLTK data directory\n",
    "nltk_data_dir = '/Users/jasper/nltk_data'\n",
    "print(os.listdir(nltk_data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['punkt', 'punkt_tab', 'punkt.zip', 'punkt_tab.zip']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "nltk_data_dir = '/Users/jasper/nltk_data/tokenizers'\n",
    "print(os.listdir(nltk_data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello welcome to the world of programming.', 'Programming is fun and interesting.', 'Please do watch the entire course to get a good understanding']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jasper/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "\n",
    "# Set the NLTK_DATA environment variable\n",
    "os.environ['NLTK_DATA'] = '/Users/jasper/nltk_data'\n",
    "\n",
    "# Append the NLTK data path\n",
    "nltk.data.path.append('/Users/jasper/nltk_data')\n",
    "\n",
    "# Ensure the 'punkt' package is downloaded\n",
    "nltk.download('punkt', download_dir='/Users/jasper/nltk_data')\n",
    "\n",
    "corpus = \"Hello welcome to the world of programming. Programming is fun and interesting. Please do watch the entire course to get a good understanding\"\n",
    "\n",
    "## Tokenization\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "my_sentences = tokenizer.tokenize(corpus)\n",
    "print(my_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome to the world of programming.\n",
      "Programming is fun and interesting.\n",
      "Please do watch the entire course to get a good understanding\n"
     ]
    }
   ],
   "source": [
    "for sentence in my_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', 'to', 'the', 'world', 'of', 'programming', '.', 'Programming', 'is', 'fun', 'and', 'interesting', '.', 'Please', 'do', 'watch', 'the', 'entire', 'course', 'to', 'get', 'a', 'good', 'understanding']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Set the NLTK_DATA environment variable to your working directory\n",
    "os.environ['NLTK_DATA'] = '/Users/jasper/Desktop/Tokenization-basics'\n",
    "\n",
    "# Append the NLTK data path\n",
    "nltk.data.path.append('/Users/jasper/Desktop/Tokenization-basics')\n",
    "\n",
    "# Manually load the punkt tokenizer\n",
    "nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "corpus = \"Hello welcome to the world of programming. Programming is fun and interesting. Please do watch the entire course to get a good understanding\"\n",
    "\n",
    "## Tokenization\n",
    "words = word_tokenize(corpus)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m(corpus)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m      4\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(sentence)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "documents = sent_tokenize(corpus)\n",
    "\n",
    "for sentence in documents:\n",
    "    words = word_tokenize(sentence)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'programming',\n",
       " '.',\n",
       " 'Programming',\n",
       " 'is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'interesting',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'good',\n",
       " 'understanding']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'programming.',\n",
       " 'Programming',\n",
       " 'is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'interesting.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'good',\n",
       " 'understanding']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a fundamental technique in Natural Language Processing (NLP) that focuses on reducing words to their base or root form, known as the \"stem.\" This process is essential for various NLP applications, including information retrieval, text analysis, and sentiment analysis. By simplifying words to their core components, stemming helps algorithms understand the underlying meaning of different word forms, which enhances the efficiency and effectiveness of language processing tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of Stemming\n",
    "\n",
    "The primary goal of stemming is to standardize words by removing prefixes and suffixes, thereby grouping related words together. For instance, the words \"running,\" \"runner,\" and \"ran\" can all be reduced to the stem \"run.\" This reduction allows NLP systems to treat these variations as equivalent, facilitating better understanding and retrieval of information.\n",
    "Stemming algorithms typically employ heuristic methods to achieve this reduction. One of the most widely used stemming algorithms is the Porter Stemmer, which applies a series of rules to trim word endings based on common suffix patterns. While stemming can significantly improve processing speed and reduce vocabulary size, it can also lead to challenges such as over-stemming (where distinct words are reduced to the same stem) and under-stemming (where similar words are not grouped together)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance in NLP:\n",
    "\n",
    "Stemming plays a crucial role in enhancing the performance of NLP applications by:\n",
    "\n",
    "1: Reducing Complexity: By normalizing variations of words, stemming simplifies the linguistic data that algorithms must process.\n",
    "\n",
    "2: Improving Search Efficiency: In information retrieval systems, stemming helps match user queries with relevant documents by recognizing different forms of a word as equivalent.\n",
    "\n",
    "3: Facilitating Text Analysis: Stemming aids in identifying patterns and relationships within text data, making it easier to conduct tasks like sentiment analysis or topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['running', 'runner', 'ran', 'runs', 'easily', 'fairly', \"eating\", 'eat', 'eater', 'ate', 'eats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running-->run\n",
      "runner-->runner\n",
      "ran-->ran\n",
      "runs-->run\n",
      "easily-->easili\n",
      "fairly-->fairli\n",
      "eating-->eat\n",
      "eat-->eat\n",
      "eater-->eater\n",
      "ate-->ate\n",
      "eats-->eat\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-->\" + stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The disadvantage of PorterStemming is that the root of the word can change\n",
    "# For example: \"easily\" becomes \"easili\" and \"fairly\" becomes \"fairli\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegexStemmer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RegexpStemmer class in Natural Language Processing (NLP) is a specialized stemming tool that utilizes regular expressions to identify and remove morphological affixes from words. This approach provides a flexible and customizable method for stemming, allowing users to define specific patterns that dictate how words should be reduced to their root forms. Unlike traditional stemming algorithms, which apply fixed rules, the RegexpStemmer enables the creation of tailored stemming rules that can accommodate the unique linguistic characteristics of different datasets or applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Features of RegexpStemmer\n",
    "\n",
    "1: Customizable Patterns: Users can specify regular expressions to match various suffixes or prefixes they wish to remove from words. This feature is particularly beneficial in applications where standard stemming algorithms may not adequately address specific linguistic nuances.\n",
    "\n",
    "2: Morphological Affix Removal: The primary function of the RegexpStemmer is to strip away affixes from words based on the defined patterns. For example, a pattern like r'ing$|s$|e$|able$ would effectively remove common endings such as \"ing,\" \"s,\" \"e,\" and \"able\" from words, simplifying them to their base forms.\n",
    "\n",
    "3: Minimum Length Parameter: The RegexpStemmer includes a parameter that allows users to set a minimum length for words to be stemmed. This helps prevent the stemming of very short words that may not benefit from reduction, thus preserving their integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance in NLP\n",
    "\n",
    "The RegexpStemmer is particularly useful in scenarios where standard stemming techniques may lead to over-stemming or under-stemming issues. By allowing for customized rules, it enhances the accuracy of text analysis tasks such as information retrieval, sentiment analysis, and document classification. This flexibility makes it an invaluable tool for researchers and developers who require precise control over how words are processed in their NLP applications.\n",
    "\n",
    "In summary, the RegexpStemmer class offers a powerful and adaptable approach to stemming in NLP, enabling users to tailor the stemming process to meet specific linguistic needs while improving the overall effectiveness of language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runn'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('running')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SnowballStemmer is an advanced stemming algorithm used in Natural Language Processing (NLP) that improves upon the original Porter Stemmer, also known as Porter2. Developed by Martin Porter, the SnowballStemmer addresses some of the limitations found in earlier stemming algorithms by providing a more robust and consistent approach to reducing words to their base or root forms. This capability is essential for various NLP tasks, including information retrieval, text classification, and sentiment analysis.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "1: Multi-Language Support: One of the standout features of the SnowballStemmer is its ability to handle multiple languages. It supports a wide range of languages including English, Spanish, French, German, and many others, making it a versatile tool for global applications.\n",
    "\n",
    "2: Improved Accuracy: The SnowballStemmer employs a more comprehensive set of rules for suffix removal compared to its predecessor. This results in more accurate stemming outcomes, reducing the likelihood of over-stemming or under-stemming—issues where distinct words are incorrectly reduced to the same stem or similar words are not grouped together.\n",
    "\n",
    "3: Customizability: Users can customize the stemming process by defining specific rules for different languages or datasets. This flexibility allows for better adaptation to the nuances of various linguistic contexts.\n",
    "\n",
    "Importance in NLP:\n",
    "\n",
    "The SnowballStemmer plays a crucial role in enhancing the performance of NLP applications by ensuring that different forms of a word are treated as equivalent. For instance, it effectively maps variations like \"running,\" \"ran,\" and \"runner\" to their common stem \"run.\" This normalization simplifies text data and improves search efficiency by allowing systems to retrieve relevant documents regardless of the specific word forms used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running-->run\n",
      "runner-->runner\n",
      "ran-->ran\n",
      "runs-->run\n",
      "easily-->easili\n",
      "fairly-->fair\n",
      "eating-->eat\n",
      "eat-->eat\n",
      "eater-->eater\n",
      "ate-->ate\n",
      "eats-->eat\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-->\" + snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sporingli')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('fairly'), stemming.stem('sporingly')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'fairly', 'fair')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('fairly'),reg_stemmer.stem('fairly'), snowball_stemmer.stem('fairly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sportingli', 'sportingly', 'sport')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('sportingly'),reg_stemmer.stem('sportingly'), snowball_stemmer.stem('sportingly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a crucial technique in Natural Language Processing (NLP) that involves reducing words to their base or dictionary form, known as the \"lemma.\" Unlike stemming, which simply truncates words to their root forms without considering context or meaning, lemmatization takes into account the intended meaning and part of speech of a word. This process ensures that words with similar meanings are grouped together, facilitating a more accurate understanding of language.\n",
    "\n",
    "Understanding Lemmatization:\n",
    "\n",
    "In lemmatization, each word is analyzed in its context to determine its lemma. For instance, the word \"better\" is lemmatized to \"good,\" reflecting its meaning based on its use in a sentence. Similarly, verbs like \"running,\" \"ran,\" and \"runs\" are all reduced to their base form \"run.\" This method is particularly useful in applications where semantic accuracy is essential, such as chatbots, search engines, and information retrieval systems.\n",
    "\n",
    "Importance in NLP:\n",
    "\n",
    "Lemmatization enhances the effectiveness of various NLP tasks by providing a more nuanced approach to text normalization. By ensuring that different forms of a word are recognized as equivalent based on their meanings, it improves the performance of algorithms in tasks such as sentiment analysis, text classification, and machine translation. Although lemmatization can be more computationally intensive than stemming due to its reliance on dictionaries and morphological analysis, its accuracy makes it invaluable for applications that require a deep understanding of language.\n",
    "In summary, lemmatization plays a vital role in NLP by enabling systems to process language with greater precision. Its ability to accurately identify and group words based on their meanings underscores its significance in developing intelligent language processing applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"going\", pos=\"n\")\n",
    "lemmatizer.lemmatize(\"going\", pos=\"a\")\n",
    "lemmatizer.lemmatize(\"going\", pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['running', 'runner', 'programming', 'programs', 'ran', 'runs', 'easily', 'fairly', \"eating\", 'eat', 'eater', 'ate', 'eats']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running-->run\n",
      "runner-->runner\n",
      "programming-->program\n",
      "programs-->program\n",
      "ran-->run\n",
      "runs-->run\n",
      "easily-->easily\n",
      "fairly-->fairly\n",
      "eating-->eat\n",
      "eat-->eat\n",
      "eater-->eater\n",
      "ate-->eat\n",
      "eats-->eat\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-->\" + lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech of Matin Luther King Jr \"I have a dream\"\n",
    "\n",
    "speech = \"\"\"I am happy to join with you today in what will go down in history as the greatest\n",
    "demonstration for freedom in the history of our nation.\n",
    "Five score years ago, a great American, in whose symbolic shadow we stand today, signed the\n",
    "Emancipation Proclamation. This momentous decree came as a great beacon light of hope to millions of\n",
    "Negro slaves who had been seared in the flames of withering injustice. It came as a joyous daybreak to\n",
    "end the long night of their captivity.\n",
    "But 100 years later, the Negro still is not free. There are those who are asking the devotees of Civil\n",
    "Rights: “When will you be satisfied?” We can never be satisfied as long as the Negro is the victim of the\n",
    "unspeakable horrors of police brutality. We can never be satisfied as long as our children are stripped of\n",
    "their selfhood and robbed of their dignity by signs stating “For whites only.” No, no we are not satisfied,\n",
    "and we will not be satisfied until “justice rolls down like waters and righteousness like a mighty stream.”\n",
    "I have a dream that one day on the red hills of Georgia, sons of former slaves and the sons of former\n",
    "slave owners will be able to sit down together at the table of brotherhood.\n",
    "I have a dream that one day, even the state of Mississippi, a state sweltering with the heat of injustice,\n",
    "sweltering with the heat of oppression, be transformed into an oasis of freedom and justice.\n",
    "I have a dream that my four little children will one day live in a nation where they will not be judged by\n",
    "the color of their skin but by the content of their character. I have a dream today.\n",
    "I have a dream that one day down in Alabama with its vicious racists, with its governor having his lips\n",
    "dripping with the words of interposition and nullification, one day right down in Alabama little black\n",
    "boys and black girls will be able to join hands with little white boys and white girls as sisters and\n",
    "brothers. I have a dream today.\n",
    "I have a dream that one day “every valley shall be exalted, every hill and mountain shall be made low,\n",
    "the rough places will be made plain, and the crooked places will be made straight, and the glory of the\n",
    "Lord shall be revealed, and all flesh shall see it together.”\n",
    "This is our hope, this is the faith that I go back to the South with. With this faith, we will be able to hew\n",
    "out of the mountain of despair, a stone of hope. With this faith, we will be able to transform the\n",
    "jangling discords of our nation into a beautiful symphony of brotherhood. With this faith we will be able\n",
    "to work together, to pray together, to struggle together, to go to jail together, to stand up for freedom\n",
    "together, knowing that we will be free one day. This will be the day. This will be the day when all of\n",
    "God’s children will be able to sing with new meaning:\n",
    "My country, ‘tis of thee, sweet land of liberty, of thee I sing.\n",
    "Land where my fathers died, land of the pilgrim’s pride,\n",
    "From every mountainside, let freedom ring!\n",
    "And if America is to be a great nation, this must become true.\n",
    "Video Transcript for Archival Research Catalog (ARC) Identifier 2602934\n",
    "National Archives and Records Administration www.archives.gov\n",
    "So let freedom ring from the prodigious hilltops of New Hampshire.\n",
    "Let freedom ring from the mighty mountains of New York.\n",
    "Let freedom ring from the heightening Alleghenies of Pennsylvania.\n",
    "Let freedom ring from the snowcapped Rockies of Colorado.\n",
    "Let freedom ring from the curvaceous slopes of California.\n",
    "But not only that. Let freedom ring from Stone Mountain of Georgia.\n",
    "Let freedom ring from Lookout Mountain of Tennessee.\n",
    "Let freedom ring from every hill and molehill of Mississippi.\n",
    "From every mountainside, let freedom ring.\n",
    "And when this happens, when we allow freedom ring, when we let it ring from every village and every\n",
    "hamlet, from every state and every city, we will be able to speed up that day when all of God’s children,\n",
    "black men and white men, Jews and Gentiles, Protestants and Catholics, will be able to join hands and\n",
    "sing in the words of the old Negro spiritual:\n",
    "Free at last! Free at last!\n",
    "Thank God Almighty, we are free at last.\n",
    "[Applause and singing]\n",
    "Speaker: On behalf of the National Committee on the March on Washington…\n",
    "Woman singing:\n",
    "Oh deep in my heart, I do believe, we shall overcome some day.\n",
    "White man work together, black man work together. We shall overcome some day!\n",
    "Oh, deep in my heart, I do believe, we shall overcome some day.\n",
    "Crowd singing:\n",
    "We’ll walk hand in hand, we’ll walk hand in hand some day!\n",
    "Oh, deep in my heart, I do believe.\n",
    "A. Philip Randolph: I think history was written today which will have its effect on coming generations,\n",
    "with respect to our democracy, with respect to our ideals, with respect to the great struggle of man,\n",
    "God, freedom, and human dignity.\n",
    "Narrator: There were many who praised this day and said that there had been a new awakening in the\n",
    "conscience of the nation. Others called it a national disgrace. In the wake of this day, more violence\n",
    "was to come, more hatred, but in the long history of man’s cruelty to man, this was a day of hope.\n",
    "Video Transcript for Archival Research Catalog (ARC) Identifier 2602934\n",
    "National Archives and Records Administration www.archives.gov\n",
    "Crowd singing:\n",
    "Freedom, freedom, freedom, freedom freedom!\n",
    "Freedom, freedom, freedom, freedom freedom!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy to join with you today in what will go down in history as the greatest\n",
      "demonstration for freedom in the history of our nation.\n",
      "Five score years ago, a great American, in whose symbolic shadow we stand today, signed the\n",
      "Emancipation Proclamation. This momentous decree came as a great beacon light of hope to millions of\n",
      "Negro slaves who had been seared in the flames of withering injustice. It came as a joyous daybreak to\n",
      "end the long night of their captivity.\n",
      "But 100 years later, the Negro still is not free. There are those who are asking the devotees of Civil\n",
      "Rights: “When will you be satisfied?” We can never be satisfied as long as the Negro is the victim of the\n",
      "unspeakable horrors of police brutality. We can never be satisfied as long as our children are stripped of\n",
      "their selfhood and robbed of their dignity by signs stating “For whites only.” No, no we are not satisfied,\n",
      "and we will not be satisfied until “justice rolls down like waters and righteousness like a mighty stream.”\n",
      "I have a dream that one day on the red hills of Georgia, sons of former slaves and the sons of former\n",
      "slave owners will be able to sit down together at the table of brotherhood.\n",
      "I have a dream that one day, even the state of Mississippi, a state sweltering with the heat of injustice,\n",
      "sweltering with the heat of oppression, be transformed into an oasis of freedom and justice.\n",
      "I have a dream that my four little children will one day live in a nation where they will not be judged by\n",
      "the color of their skin but by the content of their character. I have a dream today.\n",
      "I have a dream that one day down in Alabama with its vicious racists, with its governor having his lips\n",
      "dripping with the words of interposition and nullification, one day right down in Alabama little black\n",
      "boys and black girls will be able to join hands with little white boys and white girls as sisters and\n",
      "brothers. I have a dream today.\n",
      "I have a dream that one day “every valley shall be exalted, every hill and mountain shall be made low,\n",
      "the rough places will be made plain, and the crooked places will be made straight, and the glory of the\n",
      "Lord shall be revealed, and all flesh shall see it together.”\n",
      "This is our hope, this is the faith that I go back to the South with. With this faith, we will be able to hew\n",
      "out of the mountain of despair, a stone of hope. With this faith, we will be able to transform the\n",
      "jangling discords of our nation into a beautiful symphony of brotherhood. With this faith we will be able\n",
      "to work together, to pray together, to struggle together, to go to jail together, to stand up for freedom\n",
      "together, knowing that we will be free one day. This will be the day. This will be the day when all of\n",
      "God’s children will be able to sing with new meaning:\n",
      "My country, ‘tis of thee, sweet land of liberty, of thee I sing.\n",
      "Land where my fathers died, land of the pilgrim’s pride,\n",
      "From every mountainside, let freedom ring!\n",
      "And if America is to be a great nation, this must become true.\n",
      "Video Transcript for Archival Research Catalog (ARC) Identifier 2602934\n",
      "National Archives and Records Administration www.archives.gov\n",
      "So let freedom ring from the prodigious hilltops of New Hampshire.\n",
      "Let freedom ring from the mighty mountains of New York.\n",
      "Let freedom ring from the heightening Alleghenies of Pennsylvania.\n",
      "Let freedom ring from the snowcapped Rockies of Colorado.\n",
      "Let freedom ring from the curvaceous slopes of California.\n",
      "But not only that. Let freedom ring from Stone Mountain of Georgia.\n",
      "Let freedom ring from Lookout Mountain of Tennessee.\n",
      "Let freedom ring from every hill and molehill of Mississippi.\n",
      "From every mountainside, let freedom ring.\n",
      "And when this happens, when we allow freedom ring, when we let it ring from every village and every\n",
      "hamlet, from every state and every city, we will be able to speed up that day when all of God’s children,\n",
      "black men and white men, Jews and Gentiles, Protestants and Catholics, will be able to join hands and\n",
      "sing in the words of the old Negro spiritual:\n",
      "Free at last! Free at last!\n",
      "Thank God Almighty, we are free at last.\n",
      "[Applause and singing]\n",
      "Speaker: On behalf of the National Committee on the March on Washington…\n",
      "Woman singing:\n",
      "Oh deep in my heart, I do believe, we shall overcome some day.\n",
      "White man work together, black man work together. We shall overcome some day!\n",
      "Oh, deep in my heart, I do believe, we shall overcome some day.\n",
      "Crowd singing:\n",
      "We’ll walk hand in hand, we’ll walk hand in hand some day!\n",
      "Oh, deep in my heart, I do believe.\n",
      "A. Philip Randolph: I think history was written today which will have its effect on coming generations,\n",
      "with respect to our democracy, with respect to our ideals, with respect to the great struggle of man,\n",
      "God, freedom, and human dignity.\n",
      "Narrator: There were many who praised this day and said that there had been a new awakening in the\n",
      "conscience of the nation. Others called it a national disgrace. In the wake of this day, more violence\n",
      "was to come, more hatred, but in the long history of man’s cruelty to man, this was a day of hope.\n",
      "Video Transcript for Archival Research Catalog (ARC) Identifier 2602934\n",
      "National Archives and Records Administration www.archives.gov\n",
      "Crowd singing:\n",
      "Freedom, freedom, freedom, freedom freedom!\n",
      "Freedom, freedom, freedom, freedom freedom!\n"
     ]
    }
   ],
   "source": [
    "print(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jasper/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('dutch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "sentences = nltk.sent_tokenize(speech)\n",
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am happy to join with you today in what will go down in history as the greatest\\ndemonstration for freedom in the history of our nation.', 'Five score years ago, a great American, in whose symbolic shadow we stand today, signed the\\nEmancipation Proclamation.', 'This momentous decree came as a great beacon light of hope to millions of\\nNegro slaves who had been seared in the flames of withering injustice.', 'It came as a joyous daybreak to\\nend the long night of their captivity.', 'But 100 years later, the Negro still is not free.', 'There are those who are asking the devotees of Civil\\nRights: “When will you be satisfied?” We can never be satisfied as long as the Negro is the victim of the\\nunspeakable horrors of police brutality.', 'We can never be satisfied as long as our children are stripped of\\ntheir selfhood and robbed of their dignity by signs stating “For whites only.” No, no we are not satisfied,\\nand we will not be satisfied until “justice rolls down like waters and righteousness like a mighty stream.”\\nI have a dream that one day on the red hills of Georgia, sons of former slaves and the sons of former\\nslave owners will be able to sit down together at the table of brotherhood.', 'I have a dream that one day, even the state of Mississippi, a state sweltering with the heat of injustice,\\nsweltering with the heat of oppression, be transformed into an oasis of freedom and justice.', 'I have a dream that my four little children will one day live in a nation where they will not be judged by\\nthe color of their skin but by the content of their character.', 'I have a dream today.', 'I have a dream that one day down in Alabama with its vicious racists, with its governor having his lips\\ndripping with the words of interposition and nullification, one day right down in Alabama little black\\nboys and black girls will be able to join hands with little white boys and white girls as sisters and\\nbrothers.', 'I have a dream today.', 'I have a dream that one day “every valley shall be exalted, every hill and mountain shall be made low,\\nthe rough places will be made plain, and the crooked places will be made straight, and the glory of the\\nLord shall be revealed, and all flesh shall see it together.”\\nThis is our hope, this is the faith that I go back to the South with.', 'With this faith, we will be able to hew\\nout of the mountain of despair, a stone of hope.', 'With this faith, we will be able to transform the\\njangling discords of our nation into a beautiful symphony of brotherhood.', 'With this faith we will be able\\nto work together, to pray together, to struggle together, to go to jail together, to stand up for freedom\\ntogether, knowing that we will be free one day.', 'This will be the day.', 'This will be the day when all of\\nGod’s children will be able to sing with new meaning:\\nMy country, ‘tis of thee, sweet land of liberty, of thee I sing.', 'Land where my fathers died, land of the pilgrim’s pride,\\nFrom every mountainside, let freedom ring!', 'And if America is to be a great nation, this must become true.', 'Video Transcript for Archival Research Catalog (ARC) Identifier 2602934\\nNational Archives and Records Administration www.archives.gov\\nSo let freedom ring from the prodigious hilltops of New Hampshire.', 'Let freedom ring from the mighty mountains of New York.', 'Let freedom ring from the heightening Alleghenies of Pennsylvania.', 'Let freedom ring from the snowcapped Rockies of Colorado.', 'Let freedom ring from the curvaceous slopes of California.', 'But not only that.', 'Let freedom ring from Stone Mountain of Georgia.', 'Let freedom ring from Lookout Mountain of Tennessee.', 'Let freedom ring from every hill and molehill of Mississippi.', 'From every mountainside, let freedom ring.', 'And when this happens, when we allow freedom ring, when we let it ring from every village and every\\nhamlet, from every state and every city, we will be able to speed up that day when all of God’s children,\\nblack men and white men, Jews and Gentiles, Protestants and Catholics, will be able to join hands and\\nsing in the words of the old Negro spiritual:\\nFree at last!', 'Free at last!', 'Thank God Almighty, we are free at last.', '[Applause and singing]\\nSpeaker: On behalf of the National Committee on the March on Washington…\\nWoman singing:\\nOh deep in my heart, I do believe, we shall overcome some day.', 'White man work together, black man work together.', 'We shall overcome some day!', 'Oh, deep in my heart, I do believe, we shall overcome some day.', 'Crowd singing:\\nWe’ll walk hand in hand, we’ll walk hand in hand some day!', 'Oh, deep in my heart, I do believe.', 'A. Philip Randolph: I think history was written today which will have its effect on coming generations,\\nwith respect to our democracy, with respect to our ideals, with respect to the great struggle of man,\\nGod, freedom, and human dignity.', 'Narrator: There were many who praised this day and said that there had been a new awakening in the\\nconscience of the nation.', 'Others called it a national disgrace.', 'In the wake of this day, more violence\\nwas to come, more hatred, but in the long history of man’s cruelty to man, this was a day of hope.', 'Video Transcript for Archival Research Catalog (ARC) Identifier 2602934\\nNational Archives and Records Administration www.archives.gov\\nCrowd singing:\\nFreedom, freedom, freedom, freedom freedom!', 'Freedom, freedom, freedom, freedom freedom!']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove stopwords and then apply the stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word)for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i happi join today go histori greatest demonstr freedom histori nation .', 'five score year ago , great american , whose symbol shadow stand today , sign emancip proclam .', 'thi moment decre came great beacon light hope million negro slave sear flame wither injustic .', 'it came joyou daybreak end long night captiv .', 'but 100 year later , negro still free .', 'there ask devote civil right : “ when satisfi ? ” we never satisfi long negro victim unspeak horror polic brutal .', 'we never satisfi long children strip selfhood rob digniti sign state “ for white only. ” no , satisfi , satisfi “ justic roll like water righteous like mighti stream. ” i dream one day red hill georgia , son former slave son former slave owner abl sit togeth tabl brotherhood .', 'i dream one day , even state mississippi , state swelter heat injustic , swelter heat oppress , transform oasi freedom justic .', 'i dream four littl children one day live nation judg color skin content charact .', 'i dream today .', 'i dream one day alabama viciou racist , governor lip drip word interposit nullif , one day right alabama littl black boy black girl abl join hand littl white boy white girl sister brother .', 'i dream today .', 'i dream one day “ everi valley shall exalt , everi hill mountain shall made low , rough place made plain , crook place made straight , glori lord shall reveal , flesh shall see together. ” thi hope , faith i go back south .', 'with faith , abl hew mountain despair , stone hope .', 'with faith , abl transform jangl discord nation beauti symphoni brotherhood .', 'with faith abl work togeth , pray togeth , struggl togeth , go jail togeth , stand freedom togeth , know free one day .', 'thi day .', 'thi day god ’ children abl sing new mean : my countri , ‘ ti thee , sweet land liberti , thee i sing .', 'land father die , land pilgrim ’ pride , from everi mountainsid , let freedom ring !', 'and america great nation , must becom true .', 'video transcript archiv research catalog ( arc ) identifi 2602934 nation archiv record administr www.archives.gov so let freedom ring prodigi hilltop new hampshir .', 'let freedom ring mighti mountain new york .', 'let freedom ring heighten allegheni pennsylvania .', 'let freedom ring snowcap rocki colorado .', 'let freedom ring curvac slope california .', 'but .', 'let freedom ring stone mountain georgia .', 'let freedom ring lookout mountain tennesse .', 'let freedom ring everi hill molehil mississippi .', 'from everi mountainsid , let freedom ring .', 'and happen , allow freedom ring , let ring everi villag everi hamlet , everi state everi citi , abl speed day god ’ children , black men white men , jew gentil , protest cathol , abl join hand sing word old negro spiritu : free last !', 'free last !', 'thank god almighti , free last .', '[ applaus sing ] speaker : on behalf nation committe march washington… woman sing : oh deep heart , i believ , shall overcom day .', 'white man work togeth , black man work togeth .', 'we shall overcom day !', 'oh , deep heart , i believ , shall overcom day .', 'crowd sing : we ’ walk hand hand , ’ walk hand hand day !', 'oh , deep heart , i believ .', 'a. philip randolph : i think histori written today effect come gener , respect democraci , respect ideal , respect great struggl man , god , freedom , human digniti .', 'narrat : there mani prais day said new awaken conscienc nation .', 'other call nation disgrac .', 'in wake day , violenc come , hatr , long histori man ’ cruelti man , day hope .', 'video transcript archiv research catalog ( arc ) identifi 2602934 nation archiv record administr www.archives.gov crowd sing : freedom , freedom , freedom , freedom freedom !', 'freedom , freedom , freedom , freedom freedom !']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i happi join today go histori greatest demonstr freedom histori nation .',\n",
       " 'five score year ago , great american , whose symbol shadow stand today , sign emancip proclam .',\n",
       " 'thi moment decre came great beacon light hope million negro slave sear flame wither injustic .',\n",
       " 'it came joyou daybreak end long night captiv .',\n",
       " 'but 100 year later , negro still free .',\n",
       " 'there ask devote civil right : “ when satisfi ? ” we never satisfi long negro victim unspeak horror polic brutal .',\n",
       " 'we never satisfi long children strip selfhood rob digniti sign state “ for white only. ” no , satisfi , satisfi “ justic roll like water righteous like mighti stream. ” i dream one day red hill georgia , son former slave son former slave owner abl sit togeth tabl brotherhood .',\n",
       " 'i dream one day , even state mississippi , state swelter heat injustic , swelter heat oppress , transform oasi freedom justic .',\n",
       " 'i dream four littl children one day live nation judg color skin content charact .',\n",
       " 'i dream today .',\n",
       " 'i dream one day alabama viciou racist , governor lip drip word interposit nullif , one day right alabama littl black boy black girl abl join hand littl white boy white girl sister brother .',\n",
       " 'i dream today .',\n",
       " 'i dream one day “ everi valley shall exalt , everi hill mountain shall made low , rough place made plain , crook place made straight , glori lord shall reveal , flesh shall see together. ” thi hope , faith i go back south .',\n",
       " 'with faith , abl hew mountain despair , stone hope .',\n",
       " 'with faith , abl transform jangl discord nation beauti symphoni brotherhood .',\n",
       " 'with faith abl work togeth , pray togeth , struggl togeth , go jail togeth , stand freedom togeth , know free one day .',\n",
       " 'thi day .',\n",
       " 'thi day god ’ children abl sing new mean : my countri , ‘ ti thee , sweet land liberti , thee i sing .',\n",
       " 'land father die , land pilgrim ’ pride , from everi mountainsid , let freedom ring !',\n",
       " 'and america great nation , must becom true .',\n",
       " 'video transcript archiv research catalog ( arc ) identifi 2602934 nation archiv record administr www.archives.gov so let freedom ring prodigi hilltop new hampshir .',\n",
       " 'let freedom ring mighti mountain new york .',\n",
       " 'let freedom ring heighten allegheni pennsylvania .',\n",
       " 'let freedom ring snowcap rocki colorado .',\n",
       " 'let freedom ring curvac slope california .',\n",
       " 'but .',\n",
       " 'let freedom ring stone mountain georgia .',\n",
       " 'let freedom ring lookout mountain tennesse .',\n",
       " 'let freedom ring everi hill molehil mississippi .',\n",
       " 'from everi mountainsid , let freedom ring .',\n",
       " 'and happen , allow freedom ring , let ring everi villag everi hamlet , everi state everi citi , abl speed day god ’ children , black men white men , jew gentil , protest cathol , abl join hand sing word old negro spiritu : free last !',\n",
       " 'free last !',\n",
       " 'thank god almighti , free last .',\n",
       " '[ applaus sing ] speaker : on behalf nation committe march washington… woman sing : oh deep heart , i believ , shall overcom day .',\n",
       " 'white man work togeth , black man work togeth .',\n",
       " 'we shall overcom day !',\n",
       " 'oh , deep heart , i believ , shall overcom day .',\n",
       " 'crowd sing : we ’ walk hand hand , ’ walk hand hand day !',\n",
       " 'oh , deep heart , i believ .',\n",
       " 'a. philip randolph : i think histori written today effect come gener , respect democraci , respect ideal , respect great struggl man , god , freedom , human digniti .',\n",
       " 'narrat : there mani prais day said new awaken conscienc nation .',\n",
       " 'other call nation disgrac .',\n",
       " 'in wake day , violenc come , hatr , long histori man ’ cruelti man , day hope .',\n",
       " 'video transcript archiv research catalog ( arc ) identifi 2602934 nation archiv record administr www.archives.gov crowd sing : freedom , freedom , freedom , freedom freedom !',\n",
       " 'freedom , freedom , freedom , freedom freedom !']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')# for English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [snowball_stemmer.stem(word)for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happi join today go histori greatest demonstr freedom histori nation .',\n",
       " 'five score year ago , great american , whose symbol shadow stand today , sign emancip proclam .',\n",
       " 'thi moment decr came great beacon light hope million negro slave sear flame wither injust .',\n",
       " 'came joyou daybreak end long night captiv .',\n",
       " '100 year later , negro still free .',\n",
       " 'ask devot civil right : “ satisfi ? ” never satisfi long negro victim unspeak horror polic brutal .',\n",
       " 'never satisfi long children strip selfhood rob digniti sign state “ white . ” , satisfi , satisfi “ justic roll like water righteous like mighti stream . ” dream one day red hill georgia , son former slave son former slave owner abl sit togeth tabl brotherhood .',\n",
       " 'dream one day , even state mississippi , state swelter heat injust , swelter heat oppress , transform oasi freedom justic .',\n",
       " 'dream four littl children one day live nation judg color skin content charact .',\n",
       " 'dream today .',\n",
       " 'dream one day alabama viciou racist , governor lip drip word interposit nullif , one day right alabama littl black boy black girl abl join hand littl white boy white girl sister brother .',\n",
       " 'dream today .',\n",
       " 'dream one day “ everi valley shall exalt , everi hill mountain shall made low , rough place made plain , crook place made straight , glori lord shall reveal , flesh shall see togeth . ” thi hope , faith go back south .',\n",
       " 'faith , abl hew mountain despair , stone hope .',\n",
       " 'faith , abl transform jangl discord nation beauti symphoni brotherhood .',\n",
       " 'faith abl work togeth , pray togeth , struggl togeth , go jail togeth , stand freedom togeth , know free one day .',\n",
       " 'thi day .',\n",
       " 'thi day god ’ children abl sing new mean : countri , ‘ ti thee , sweet land liberti , thee sing .',\n",
       " 'land father die , land pilgrim ’ pride , everi mountainsid , let freedom ring !',\n",
       " 'america great nation , must becom true .',\n",
       " 'video transcript archiv research catalog ( arc ) identifi 2602934 nation archiv record administr www.archives.gov let freedom ring prodigi hilltop new hampshir .',\n",
       " 'let freedom ring mighti mountain new york .',\n",
       " 'let freedom ring heighten allegheni pennsylvania .',\n",
       " 'let freedom ring snowcap rocki colorado .',\n",
       " 'let freedom ring curvac slope california .',\n",
       " '.',\n",
       " 'let freedom ring stone mountain georgia .',\n",
       " 'let freedom ring lookout mountain tenness .',\n",
       " 'let freedom ring everi hill molehil mississippi .',\n",
       " 'everi mountainsid , let freedom ring .',\n",
       " 'happen , allow freedom ring , let ring everi villag everi hamlet , everi state everi citi , abl speed day god ’ children , black men white men , jew gentil , protest cathol , abl join hand sing word old negro spiritu : free last !',\n",
       " 'free last !',\n",
       " 'thank god almighti , free last .',\n",
       " '[ applaus sing ] speaker : behalf nation committ march washington… woman sing : oh deep heart , believ , shall overcom day .',\n",
       " 'white man work togeth , black man work togeth .',\n",
       " 'shall overcom day !',\n",
       " 'oh , deep heart , believ , shall overcom day .',\n",
       " 'crowd sing : ’ walk hand hand , ’ walk hand hand day !',\n",
       " 'oh , deep heart , believ .',\n",
       " 'a. philip randolph : think histori written today effect come gener , respect democraci , respect ideal , respect great struggl man , god , freedom , human digniti .',\n",
       " 'narrat : mani prai day said new awaken conscienc nation .',\n",
       " 'call nation disgrac .',\n",
       " 'wake day , violenc come , hatr , long histori man ’ cruelti man , day hope .',\n",
       " 'video transcript archiv research catalog ( arc ) identifi 2602934 nation archiv record administr www.archives.gov crowd sing : freedom , freedom , freedom , freedom freedom !',\n",
       " 'freedom , freedom , freedom , freedom freedom !']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word)for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happi join today go histori greatest demonstr freedom histori nation .',\n",
       " 'five score year ago , great american , whose symbol shadow stand today , sign emancip proclam .',\n",
       " 'thi moment decr came great beacon light hope million negro slave sear flame wither injust .',\n",
       " 'came joyou daybreak end long night captiv .',\n",
       " '100 year later , negro still free .',\n",
       " 'ask devot civil right : “ satisfi ? ” never satisfi long negro victim unspeak horror polic brutal .',\n",
       " 'never satisfi long child strip selfhood rob digniti sign state “ white . ” , satisfi , satisfi “ justic roll like water righteous like mighti stream . ” dream one day red hill georgia , son former slave son former slave owner abl sit togeth tabl brotherhood .',\n",
       " 'dream one day , even state mississippi , state swelter heat injust , swelter heat oppress , transform oasi freedom justic .',\n",
       " 'dream four littl child one day live nation judg color skin content charact .',\n",
       " 'dream today .',\n",
       " 'dream one day alabama viciou racist , governor lip drip word interposit nullif , one day right alabama littl black boy black girl abl join hand littl white boy white girl sister brother .',\n",
       " 'dream today .',\n",
       " 'dream one day “ everi valley shall exalt , everi hill mountain shall made low , rough place made plain , crook place made straight , glori lord shall reveal , flesh shall see togeth . ” thi hope , faith go back south .',\n",
       " 'faith , abl hew mountain despair , stone hope .',\n",
       " 'faith , abl transform jangl discord nation beauti symphoni brotherhood .',\n",
       " 'faith abl work togeth , pray togeth , struggl togeth , go jail togeth , stand freedom togeth , know free one day .',\n",
       " 'thi day .',\n",
       " 'thi day god ’ child abl sing new mean : countri , ‘ ti thee , sweet land liberti , thee sing .',\n",
       " 'land father die , land pilgrim ’ pride , everi mountainsid , let freedom ring !',\n",
       " 'america great nation , must becom true .',\n",
       " 'video transcript archiv research catalog ( arc ) identifi 2602934 nation archiv record administr www.archives.gov let freedom ring prodigi hilltop new hampshir .',\n",
       " 'let freedom ring mighti mountain new york .',\n",
       " 'let freedom ring heighten allegheni pennsylvania .',\n",
       " 'let freedom ring snowcap rocki colorado .',\n",
       " 'let freedom ring curvac slope california .',\n",
       " '.',\n",
       " 'let freedom ring stone mountain georgia .',\n",
       " 'let freedom ring lookout mountain tenness .',\n",
       " 'let freedom ring everi hill molehil mississippi .',\n",
       " 'everi mountainsid , let freedom ring .',\n",
       " 'happen , allow freedom ring , let ring everi villag everi hamlet , everi state everi citi , abl speed day god ’ child , black men white men , jew gentil , protest cathol , abl join hand sing word old negro spiritu : free last !',\n",
       " 'free last !',\n",
       " 'thank god almighti , free last .',\n",
       " '[ applaus sing ] speaker : behalf nation committ march washington… woman sing : oh deep heart , believ , shall overcom day .',\n",
       " 'white man work togeth , black man work togeth .',\n",
       " 'shall overcom day !',\n",
       " 'oh , deep heart , believ , shall overcom day .',\n",
       " 'crowd sing : ’ walk hand hand , ’ walk hand hand day !',\n",
       " 'oh , deep heart , believ .',\n",
       " 'a. philip randolph : think histori written today effect come gener , respect democraci , respect ideal , respect great struggl man , god , freedom , human digniti .',\n",
       " 'narrat : mani prai day said new awaken conscienc nation .',\n",
       " 'call nation disgrac .',\n",
       " 'wake day , violenc come , hatr , long histori man ’ cruelti man , day hope .',\n",
       " 'video transcript archiv research catalog ( arc ) identifi 2602934 nation archiv record administr www.archives.gov crowd sing : freedom , freedom , freedom , freedom freedom !',\n",
       " 'freedom , freedom , freedom , freedom freedom !']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word.lower())for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happi join today go histori greatest demonstr freedom histori nation .',\n",
       " 'five score year ago , great american , whose symbol shadow stand today , sign emancip proclam .',\n",
       " 'thi moment decr came great beacon light hope million negro slave sear flame wither injust .',\n",
       " 'came joyou daybreak end long night captiv .',\n",
       " '100 year later , negro still free .',\n",
       " 'ask devot civil right : “ satisfi ? ” never satisfi long negro victim unspeak horror polic brutal .',\n",
       " 'never satisfi long child strip selfhood rob digniti sign state “ white . ” , satisfi , satisfi “ justic roll like water righteous like mighti stream . ” dream one day red hill georgia , son former slave son former slave owner abl sit togeth tabl brotherhood .',\n",
       " 'dream one day , even state mississippi , state swelter heat injust , swelter heat oppress , transform oasi freedom justic .',\n",
       " 'dream four littl child one day live nation judg color skin content charact .',\n",
       " 'dream today .',\n",
       " 'dream one day alabama viciou racist , governor lip drip word interposit nullif , one day right alabama littl black boy black girl abl join hand littl white boy white girl sister brother .',\n",
       " 'dream today .',\n",
       " 'dream one day “ everi valley shall exalt , everi hill mountain shall made low , rough place made plain , crook place made straight , glori lord shall reveal , flesh shall see togeth . ” thi hope , faith go back south .',\n",
       " 'faith , abl hew mountain despair , stone hope .',\n",
       " 'faith , abl transform jangl discord nation beauti symphoni brotherhood .',\n",
       " 'faith abl work togeth , pray togeth , struggl togeth , go jail togeth , stand freedom togeth , know free one day .',\n",
       " 'thi day .',\n",
       " 'thi day god ’ child abl sing new mean : countri , ‘ ti thee , sweet land liberti , thee sing .',\n",
       " 'land father die , land pilgrim ’ pride , everi mountainsid , let freedom ring !',\n",
       " 'america great nation , must becom true .',\n",
       " 'video transcript archiv research catalog ( arc ) identifi 2602934 nation archiv record administr www.archives.gov let freedom ring prodigi hilltop new hampshir .',\n",
       " 'let freedom ring mighti mountain new york .',\n",
       " 'let freedom ring heighten allegheni pennsylvania .',\n",
       " 'let freedom ring snowcap rocki colorado .',\n",
       " 'let freedom ring curvac slope california .',\n",
       " '.',\n",
       " 'let freedom ring stone mountain georgia .',\n",
       " 'let freedom ring lookout mountain tenness .',\n",
       " 'let freedom ring everi hill molehil mississippi .',\n",
       " 'everi mountainsid , let freedom ring .',\n",
       " 'happen , allow freedom ring , let ring everi villag everi hamlet , everi state everi citi , abl speed day god ’ child , black men white men , jew gentil , protest cathol , abl join hand sing word old negro spiritu : free last !',\n",
       " 'free last !',\n",
       " 'thank god almighti , free last .',\n",
       " '[ applaus sing ] speaker : behalf nation committ march washington… woman sing : oh deep heart , believ , shall overcom day .',\n",
       " 'white man work togeth , black man work togeth .',\n",
       " 'shall overcom day !',\n",
       " 'oh , deep heart , believ , shall overcom day .',\n",
       " 'crowd sing : ’ walk hand hand , ’ walk hand hand day !',\n",
       " 'oh , deep heart , believ .',\n",
       " 'a. philip randolph : think histori written today effect come gener , respect democraci , respect ideal , respect great struggl man , god , freedom , human digniti .',\n",
       " 'narrat : mani prai day said new awaken conscienc nation .',\n",
       " 'call nation disgrac .',\n",
       " 'wake day , violenc come , hatr , long histori man ’ cruelti man , day hope .',\n",
       " 'video transcript archiv research catalog ( arc ) identifi 2602934 nation archiv record administr www.archives.gov crowd sing : freedom , freedom , freedom , freedom freedom !',\n",
       " 'freedom , freedom , freedom , freedom freedom !']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is a fundamental task in Natural Language Processing (NLP) that involves assigning grammatical categories, such as nouns, verbs, adjectives, and adverbs, to each word in a given text. This process enhances the understanding of the syntactic structure and meaning of sentences by providing a layer of linguistic information that is crucial for various NLP applications, including machine translation, information extraction, and named entity recognition.\n",
    "Importance of POS Tagging\n",
    "\n",
    "The significance of POS tagging lies in its ability to disambiguate words that may have multiple meanings based on their context. For example, the word \"bark\" can refer to the sound a dog makes or the outer covering of a tree; POS tagging helps clarify its intended meaning by identifying its part of speech within the sentence. This capability is essential for accurately interpreting text and facilitating more advanced language processing tasks.\n",
    "\n",
    "Techniques for POS Tagging:\n",
    "\n",
    "POS tagging can be performed using various methods, including rule-based systems that rely on predefined grammatical rules, statistical methods that utilize machine learning algorithms trained on annotated corpora, and transformation-based approaches that combine elements of both. Each method has its strengths and weaknesses, making the choice of technique dependent on the specific requirements of the application and the characteristics of the dataset.\n",
    "\n",
    "In summary, part-of-speech tagging is a crucial step in NLP that enables algorithms to analyze and understand language more effectively. By categorizing words according to their grammatical roles, POS tagging lays the groundwork for deeper linguistic analysis and enhances the performance of numerous NLP applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech2 = \"\"\"Thank you. I'm honored to be with you today for your commencement from one of the finest universities in the world. Truth be told, I never graduated from college, and this is the closest I've ever gotten to a college graduation today. I want to tell you three stories from my life. That's it. No big deal. Just three stories. The first story is about connecting the dots. I dropped out of Reed College after the first six months, but then stayed around as a drop-in for another 18 months or so before I really quit. So why'd I drop out? It started before I was born. Steve Jobs: (01:15) My biological mother was a young, unwed graduate student and she decided to put me up for adoption. She felt very strongly that I should be adopted by college graduates. So everything was all set for me to be adopted at birth by a lawyer and his wife, except that when I popped out, they decided at the last minute that they really wanted a girl. So my parents, who were on a waiting list, got a call in the middle of the night asking \"We've got an unexpected baby boy, do you want him?\" They said, \"Of course.\" My biological mother found out later that my mother had never graduated from college and that my father had never graduated from high school. She refused to sign the final adoption papers. She only relented a few months later when my parents promised that I would go to college. This was the start in my life. Steve Jobs: (02:13) 17 years later, I did go to college, but I naively chose a college that was almost as expensive as Stanford, and all of my working class parents savings were being spent on my college tuition. After six months, I couldn't see the value in it. I had no idea what I wanted to do with my life and no idea how college was going to help me figure it out, and here I was spending all the money. My parents had saved their entire life, so I decided to drop out and trust that it would all work out. Okay. It was pretty scary at the time, but looking back, it was one of the best decisions I ever made. The minute I dropped out, I could stop taking the required classes that didn't interest me and begin dropping in on the ones that looked far more interesting. Steve Jobs: (03:04) It wasn't all romantic. I didn't have a dorm room, so I slept on the floor in friends rooms. I returned Coke bottles for the 5 cent deposits to buy food with, and I would walk the seven miles across town every Sunday night to get one good meal a week at the Hari Krishna temple. I loved it, and much of what I stumbled into by following my curiosity and intuition turned out to be priceless later on. Let me give you one example, Reed College at that time offered perhaps the best calligraphy instruction in the country throughout the campus. Every poster, every label on every drawer was beautifully hand calligraphed. Because I had dropped out and didn't have to take the normal classes, I decided to take a calligraphy class to learn how to do this. I learned about serif and sanserif typefaces, about varying the amount of space between different letter combinations, about what makes great typography great. Steve Jobs: (04:03) It was beautiful, historical, artistically subtle in a way that science can't capture, and I found it fascinating. None of this had even a hope of any practical application in my life. But 10 years later, when we were designing the first Macintosh computer, it all came back to me and we designed it all into the Mac. It was the first computer with beautiful typography. If I had never dropped in on that single course in college, the Mac would have never had multiple typefaces or proportionally spaced fonts. Since Windows just copied the Mac, it's likely that no personal computer would have them. If I had never dropped out, I would have never dropped in on that calligraphy class and personal computers might not have the wonderful typography that they do. Steve Jobs: (04:58) Of course, it was impossible to connect the dots looking forward when I was in college, but it was very, very clear looking backwards 10 years later. Again, you can't connect the dots looking forward. You can only connect them looking backwards. So you have to trust that the dots will somehow connect in your future. You have to trust in something, your gut, destiny, life, karma, whatever, because believing that the dots will connect down the road will give you the confidence to follow your heart, even when it leads you off the well-worn path, and that will make all the difference. Steve Jobs: (05:38) My second story is about love and loss. I was lucky. I found what I love to do early in life. Woz and I started Apple in my parents' garage when I was 20, we worked hard and in 10 years, Apple had grown from just the two of us in a garage, into a 2 billion company with over 4,000 employees. We just released our finest creation, the Macintosh, a year earlier, and I just turned 30, and then I got fired. How can you get fired from a company you started? Well, as Apple grew, we hired someone who I thought was very talented to run the company with me, and for the first year or so, things went well. But then our visions of the future began to diverge, and eventually we had a falling out. When we did, our board of directors sided with him. Steve Jobs: (06:28) So at 30, I was out and very publicly out. What had been the focus of my entire adult life was gone and it was devastating. I really didn't know what to do for a few months. I felt that I had let the previous generation of entrepreneurs down, that I had dropped the baton as it was being passed to me. I met with David Packard and Bob Noyce, and tried to apologize for screwing up so badly. I was a very public failure and I even thought about running away from the Valley, but something slowly began to dawn on me. I still loved what I did. Steve Jobs: (07:04) The turn of events at Apple had not changed that one bit. I'd been rejected, but I was still in love. So I decided to start over. I didn't see it then, but it turned out that getting fired from Apple was the best thing that could've ever happened to me. The heaviness of being successful was replaced by the lightness of being a beginner again, less sure about everything. It freed me to enter one of the most creative periods of my life. During the next five years, I started a company named Next, another company named Pixar, and fell in love with an amazing woman who would become my wife. Pixar went on to create the world's first computer animated feature film, Toy Story, and is now the most successful animation studio in the world. Steve Jobs: (07:49) In a remarkable turn of events, Apple bought Next and I returned to Apple, and the technology we developed at Next is at the heart of Apple's current renaissance. Laurene and I have a wonderful family together. I'm pretty sure none of this would have happened if I hadn't been fired from Apple. It was awful tasting medicine, but I guess the patient needed it. Sometimes life's going to hit you in the head with a brick. Don't lose faith. I'm convinced that the only thing that kept me going was that I loved what I did. Steve Jobs: (08:21) You've got to find what you love, and that is as true for work as it is for your lovers. Your work is going to fill a large part of your life, and the only way to be truly satisfied is to do what you believe is great work. The only way to do great work is to love what you do. If you haven't found it yet, keep looking and don't settle. As with all matters of the heart, you'll know when you find it and, like any great relationship, it just gets better and better as the years roll on. So keep looking. Don't settle. Steve Jobs: (09:04) My third story is about death. When I was 17, I read a quote that went something like, \"If you live each day as if it was your last, someday you'll most certainly be right.\" It made an impression on me. Since then, for the past 33 years, I've looked in the mirror every morning and asked myself, if today were the last day of my life, would I want to do what I am about to do today? Whenever the answer has been no for too many days in a row, I know I need to change something. Remembering that I'll be dead soon is the most important tool I've ever encountered to help me make the big choices in life. Because almost everything, all external expectations, all pride, all fear of embarrassment or failure, these things just fall away in the face of death, leaving only what is truly important. Remembering that you are going to die is the best way I know to avoid the trap of thinking you have something to lose. You are already naked. There is no reason not to follow your heart. Steve Jobs: (10:11) About a year ago, I was diagnosed with cancer. I had a scan at 7:30 in the morning, and it clearly showed a tumor on my pancreas. I didn't even know what a pancreas was. The doctors told me this was almost certainly a type of cancer that is incurable and that I should expect to live no longer than three to six months. My doctor advised me to go home and get my affairs in order, which is doctor's code for prepare to die. It means to try and tell your kids everything you thought you'd have the next 10 years to tell them in just a few months. It means to make sure everything is buttoned up so that it will be as easy as possible for your family. It means to say your goodbyes. Steve Jobs: (11:00) I lived with that diagnosis all day. Later that evening, I had a biopsy where they stuck an endoscope down my throat, through my stomach, and into my intestines, put a needle into my pancreas and got a few cells from the tumor. I was sedated, but my wife who was there told me that when they viewed the cells under a microscope, the doctor started crying because it turned out to be a very rare form of pancreatic cancer that is curable with surgery. I had the surgery and thankfully, I'm fine now Steve Jobs: (11:40) This was the closest I've been to facing death, and I hope it's the closest I get for a few more decades. Having lived through it, I can now say this to you with a bit more certainty than when death was a useful, but purely intellectual concept. No one wants to die. Even people who want to go to heaven don't want to die to get there. Yet, death is the destination we all share. No one has ever escaped it, and that is as it should be because death is very likely the single best invention of life. Steve Jobs: (12:14) It's life's change agent. It clears out the old to make way for the new. Right now, the new is you, but someday not too long from now, you will gradually become the old and be cleared away. Sorry to be so dramatic, but it's quite true. Your time is limited, so don't waste it living someone else's life. Don't be trapped by dogma, which is living with the results of other people's thinking. Don't let the noise of others opinions drowned out your own inner voice, and most important, have the courage to follow your heart and intuition. They somehow already know what you truly want to become. Everything else is secondary. Steve Jobs: (13:08) When I was young, there was an amazing publication called the Whole Earth Catalog, which was one of the Bibles of my generation. It was created by a fellow named Stewart Brand, not far from here in Menlo Park, and he brought it to life with his poetic touch. This was in the late 60's before personal computers and desktop publishing. So it was all made with typewriters, scissors, and Polaroid cameras. It was sort of like Google in paperback form 35 years before Google came along. It was idealistic, overflowing with neat tools, and great notions. Stewart and his team put out several issues of the Whole Earth Catalog, and then when it had run its course, they put out a final issue. Steve Jobs: (13:52) It was the mid 1970s, and I was your age. On the back cover of their final issue was a photograph of an early morning country road, the kind you might find yourself hitchhiking on if you were so adventurous. Beneath it were the words, \"Stay hungry, stay foolish.\" It was their farewell message as they signed off. Stay hungry, stay foolish. I've always wished that for myself, and now as you graduate to begin anew, I wish that for you. Stay hungry, stay foolish. Thank you all very much. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you. I'm honored to be with you today for your commencement from one of the finest universities in the world. Truth be told, I never graduated from college, and this is the closest I've ever gotten to a college graduation today. I want to tell you three stories from my life. That's it. No big deal. Just three stories. The first story is about connecting the dots. I dropped out of Reed College after the first six months, but then stayed around as a drop-in for another 18 months or so before I really quit. So why'd I drop out? It started before I was born. Steve Jobs: (01:15) My biological mother was a young, unwed graduate student and she decided to put me up for adoption. She felt very strongly that I should be adopted by college graduates. So everything was all set for me to be adopted at birth by a lawyer and his wife, except that when I popped out, they decided at the last minute that they really wanted a girl. So my parents, who were on a waiting list, got a call in the middle of the night asking \"We've got an unexpected baby boy, do you want him?\" They said, \"Of course.\" My biological mother found out later that my mother had never graduated from college and that my father had never graduated from high school. She refused to sign the final adoption papers. She only relented a few months later when my parents promised that I would go to college. This was the start in my life. Steve Jobs: (02:13) 17 years later, I did go to college, but I naively chose a college that was almost as expensive as Stanford, and all of my working class parents savings were being spent on my college tuition. After six months, I couldn't see the value in it. I had no idea what I wanted to do with my life and no idea how college was going to help me figure it out, and here I was spending all the money. My parents had saved their entire life, so I decided to drop out and trust that it would all work out. Okay. It was pretty scary at the time, but looking back, it was one of the best decisions I ever made. The minute I dropped out, I could stop taking the required classes that didn't interest me and begin dropping in on the ones that looked far more interesting. Steve Jobs: (03:04) It wasn't all romantic. I didn't have a dorm room, so I slept on the floor in friends rooms. I returned Coke bottles for the 5 cent deposits to buy food with, and I would walk the seven miles across town every Sunday night to get one good meal a week at the Hari Krishna temple. I loved it, and much of what I stumbled into by following my curiosity and intuition turned out to be priceless later on. Let me give you one example, Reed College at that time offered perhaps the best calligraphy instruction in the country throughout the campus. Every poster, every label on every drawer was beautifully hand calligraphed. Because I had dropped out and didn't have to take the normal classes, I decided to take a calligraphy class to learn how to do this. I learned about serif and sanserif typefaces, about varying the amount of space between different letter combinations, about what makes great typography great. Steve Jobs: (04:03) It was beautiful, historical, artistically subtle in a way that science can't capture, and I found it fascinating. None of this had even a hope of any practical application in my life. But 10 years later, when we were designing the first Macintosh computer, it all came back to me and we designed it all into the Mac. It was the first computer with beautiful typography. If I had never dropped in on that single course in college, the Mac would have never had multiple typefaces or proportionally spaced fonts. Since Windows just copied the Mac, it's likely that no personal computer would have them. If I had never dropped out, I would have never dropped in on that calligraphy class and personal computers might not have the wonderful typography that they do. Steve Jobs: (04:58) Of course, it was impossible to connect the dots looking forward when I was in college, but it was very, very clear looking backwards 10 years later. Again, you can't connect the dots looking forward. You can only connect them looking backwards. So you have to trust that the dots will somehow connect in your future. You have to trust in something, your gut, destiny, life, karma, whatever, because believing that the dots will connect down the road will give you the confidence to follow your heart, even when it leads you off the well-worn path, and that will make all the difference. Steve Jobs: (05:38) My second story is about love and loss. I was lucky. I found what I love to do early in life. Woz and I started Apple in my parents' garage when I was 20, we worked hard and in 10 years, Apple had grown from just the two of us in a garage, into a 2 billion company with over 4,000 employees. We just released our finest creation, the Macintosh, a year earlier, and I just turned 30, and then I got fired. How can you get fired from a company you started? Well, as Apple grew, we hired someone who I thought was very talented to run the company with me, and for the first year or so, things went well. But then our visions of the future began to diverge, and eventually we had a falling out. When we did, our board of directors sided with him. Steve Jobs: (06:28) So at 30, I was out and very publicly out. What had been the focus of my entire adult life was gone and it was devastating. I really didn't know what to do for a few months. I felt that I had let the previous generation of entrepreneurs down, that I had dropped the baton as it was being passed to me. I met with David Packard and Bob Noyce, and tried to apologize for screwing up so badly. I was a very public failure and I even thought about running away from the Valley, but something slowly began to dawn on me. I still loved what I did. Steve Jobs: (07:04) The turn of events at Apple had not changed that one bit. I'd been rejected, but I was still in love. So I decided to start over. I didn't see it then, but it turned out that getting fired from Apple was the best thing that could've ever happened to me. The heaviness of being successful was replaced by the lightness of being a beginner again, less sure about everything. It freed me to enter one of the most creative periods of my life. During the next five years, I started a company named Next, another company named Pixar, and fell in love with an amazing woman who would become my wife. Pixar went on to create the world's first computer animated feature film, Toy Story, and is now the most successful animation studio in the world. Steve Jobs: (07:49) In a remarkable turn of events, Apple bought Next and I returned to Apple, and the technology we developed at Next is at the heart of Apple's current renaissance. Laurene and I have a wonderful family together. I'm pretty sure none of this would have happened if I hadn't been fired from Apple. It was awful tasting medicine, but I guess the patient needed it. Sometimes life's going to hit you in the head with a brick. Don't lose faith. I'm convinced that the only thing that kept me going was that I loved what I did. Steve Jobs: (08:21) You've got to find what you love, and that is as true for work as it is for your lovers. Your work is going to fill a large part of your life, and the only way to be truly satisfied is to do what you believe is great work. The only way to do great work is to love what you do. If you haven't found it yet, keep looking and don't settle. As with all matters of the heart, you'll know when you find it and, like any great relationship, it just gets better and better as the years roll on. So keep looking. Don't settle. Steve Jobs: (09:04) My third story is about death. When I was 17, I read a quote that went something like, \"If you live each day as if it was your last, someday you'll most certainly be right.\" It made an impression on me. Since then, for the past 33 years, I've looked in the mirror every morning and asked myself, if today were the last day of my life, would I want to do what I am about to do today? Whenever the answer has been no for too many days in a row, I know I need to change something. Remembering that I'll be dead soon is the most important tool I've ever encountered to help me make the big choices in life. Because almost everything, all external expectations, all pride, all fear of embarrassment or failure, these things just fall away in the face of death, leaving only what is truly important. Remembering that you are going to die is the best way I know to avoid the trap of thinking you have something to lose. You are already naked. There is no reason not to follow your heart. Steve Jobs: (10:11) About a year ago, I was diagnosed with cancer. I had a scan at 7:30 in the morning, and it clearly showed a tumor on my pancreas. I didn't even know what a pancreas was. The doctors told me this was almost certainly a type of cancer that is incurable and that I should expect to live no longer than three to six months. My doctor advised me to go home and get my affairs in order, which is doctor's code for prepare to die. It means to try and tell your kids everything you thought you'd have the next 10 years to tell them in just a few months. It means to make sure everything is buttoned up so that it will be as easy as possible for your family. It means to say your goodbyes. Steve Jobs: (11:00) I lived with that diagnosis all day. Later that evening, I had a biopsy where they stuck an endoscope down my throat, through my stomach, and into my intestines, put a needle into my pancreas and got a few cells from the tumor. I was sedated, but my wife who was there told me that when they viewed the cells under a microscope, the doctor started crying because it turned out to be a very rare form of pancreatic cancer that is curable with surgery. I had the surgery and thankfully, I'm fine now Steve Jobs: (11:40) This was the closest I've been to facing death, and I hope it's the closest I get for a few more decades. Having lived through it, I can now say this to you with a bit more certainty than when death was a useful, but purely intellectual concept. No one wants to die. Even people who want to go to heaven don't want to die to get there. Yet, death is the destination we all share. No one has ever escaped it, and that is as it should be because death is very likely the single best invention of life. Steve Jobs: (12:14) It's life's change agent. It clears out the old to make way for the new. Right now, the new is you, but someday not too long from now, you will gradually become the old and be cleared away. Sorry to be so dramatic, but it's quite true. Your time is limited, so don't waste it living someone else's life. Don't be trapped by dogma, which is living with the results of other people's thinking. Don't let the noise of others opinions drowned out your own inner voice, and most important, have the courage to follow your heart and intuition. They somehow already know what you truly want to become. Everything else is secondary. Steve Jobs: (13:08) When I was young, there was an amazing publication called the Whole Earth Catalog, which was one of the Bibles of my generation. It was created by a fellow named Stewart Brand, not far from here in Menlo Park, and he brought it to life with his poetic touch. This was in the late 60's before personal computers and desktop publishing. So it was all made with typewriters, scissors, and Polaroid cameras. It was sort of like Google in paperback form 35 years before Google came along. It was idealistic, overflowing with neat tools, and great notions. Stewart and his team put out several issues of the Whole Earth Catalog, and then when it had run its course, they put out a final issue. Steve Jobs: (13:52) It was the mid 1970s, and I was your age. On the back cover of their final issue was a photograph of an early morning country road, the kind you might find yourself hitchhiking on if you were so adventurous. Beneath it were the words, \"Stay hungry, stay foolish.\" It was their farewell message as they signed off. Stay hungry, stay foolish. I've always wished that for myself, and now as you graduate to begin anew, I wish that for you. Stay hungry, stay foolish. Thank you all very much. \n"
     ]
    }
   ],
   "source": [
    "print(speech2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sentences = nltk.sent_tokenize(speech2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you.',\n",
       " \"I'm honored to be with you today for your commencement from one of the finest universities in the world.\",\n",
       " \"Truth be told, I never graduated from college, and this is the closest I've ever gotten to a college graduation today.\",\n",
       " 'I want to tell you three stories from my life.',\n",
       " \"That's it.\",\n",
       " 'No big deal.',\n",
       " 'Just three stories.',\n",
       " 'The first story is about connecting the dots.',\n",
       " 'I dropped out of Reed College after the first six months, but then stayed around as a drop-in for another 18 months or so before I really quit.',\n",
       " \"So why'd I drop out?\",\n",
       " 'It started before I was born.',\n",
       " 'Steve Jobs: (01:15) My biological mother was a young, unwed graduate student and she decided to put me up for adoption.',\n",
       " 'She felt very strongly that I should be adopted by college graduates.',\n",
       " 'So everything was all set for me to be adopted at birth by a lawyer and his wife, except that when I popped out, they decided at the last minute that they really wanted a girl.',\n",
       " 'So my parents, who were on a waiting list, got a call in the middle of the night asking \"We\\'ve got an unexpected baby boy, do you want him?\"',\n",
       " 'They said, \"Of course.\"',\n",
       " 'My biological mother found out later that my mother had never graduated from college and that my father had never graduated from high school.',\n",
       " 'She refused to sign the final adoption papers.',\n",
       " 'She only relented a few months later when my parents promised that I would go to college.',\n",
       " 'This was the start in my life.',\n",
       " 'Steve Jobs: (02:13) 17 years later, I did go to college, but I naively chose a college that was almost as expensive as Stanford, and all of my working class parents savings were being spent on my college tuition.',\n",
       " \"After six months, I couldn't see the value in it.\",\n",
       " 'I had no idea what I wanted to do with my life and no idea how college was going to help me figure it out, and here I was spending all the money.',\n",
       " 'My parents had saved their entire life, so I decided to drop out and trust that it would all work out.',\n",
       " 'Okay.',\n",
       " 'It was pretty scary at the time, but looking back, it was one of the best decisions I ever made.',\n",
       " \"The minute I dropped out, I could stop taking the required classes that didn't interest me and begin dropping in on the ones that looked far more interesting.\",\n",
       " \"Steve Jobs: (03:04) It wasn't all romantic.\",\n",
       " \"I didn't have a dorm room, so I slept on the floor in friends rooms.\",\n",
       " 'I returned Coke bottles for the 5 cent deposits to buy food with, and I would walk the seven miles across town every Sunday night to get one good meal a week at the Hari Krishna temple.',\n",
       " 'I loved it, and much of what I stumbled into by following my curiosity and intuition turned out to be priceless later on.',\n",
       " 'Let me give you one example, Reed College at that time offered perhaps the best calligraphy instruction in the country throughout the campus.',\n",
       " 'Every poster, every label on every drawer was beautifully hand calligraphed.',\n",
       " \"Because I had dropped out and didn't have to take the normal classes, I decided to take a calligraphy class to learn how to do this.\",\n",
       " 'I learned about serif and sanserif typefaces, about varying the amount of space between different letter combinations, about what makes great typography great.',\n",
       " \"Steve Jobs: (04:03) It was beautiful, historical, artistically subtle in a way that science can't capture, and I found it fascinating.\",\n",
       " 'None of this had even a hope of any practical application in my life.',\n",
       " 'But 10 years later, when we were designing the first Macintosh computer, it all came back to me and we designed it all into the Mac.',\n",
       " 'It was the first computer with beautiful typography.',\n",
       " 'If I had never dropped in on that single course in college, the Mac would have never had multiple typefaces or proportionally spaced fonts.',\n",
       " \"Since Windows just copied the Mac, it's likely that no personal computer would have them.\",\n",
       " 'If I had never dropped out, I would have never dropped in on that calligraphy class and personal computers might not have the wonderful typography that they do.',\n",
       " 'Steve Jobs: (04:58) Of course, it was impossible to connect the dots looking forward when I was in college, but it was very, very clear looking backwards 10 years later.',\n",
       " \"Again, you can't connect the dots looking forward.\",\n",
       " 'You can only connect them looking backwards.',\n",
       " 'So you have to trust that the dots will somehow connect in your future.',\n",
       " 'You have to trust in something, your gut, destiny, life, karma, whatever, because believing that the dots will connect down the road will give you the confidence to follow your heart, even when it leads you off the well-worn path, and that will make all the difference.',\n",
       " 'Steve Jobs: (05:38) My second story is about love and loss.',\n",
       " 'I was lucky.',\n",
       " 'I found what I love to do early in life.',\n",
       " \"Woz and I started Apple in my parents' garage when I was 20, we worked hard and in 10 years, Apple had grown from just the two of us in a garage, into a 2 billion company with over 4,000 employees.\",\n",
       " 'We just released our finest creation, the Macintosh, a year earlier, and I just turned 30, and then I got fired.',\n",
       " 'How can you get fired from a company you started?',\n",
       " 'Well, as Apple grew, we hired someone who I thought was very talented to run the company with me, and for the first year or so, things went well.',\n",
       " 'But then our visions of the future began to diverge, and eventually we had a falling out.',\n",
       " 'When we did, our board of directors sided with him.',\n",
       " 'Steve Jobs: (06:28) So at 30, I was out and very publicly out.',\n",
       " 'What had been the focus of my entire adult life was gone and it was devastating.',\n",
       " \"I really didn't know what to do for a few months.\",\n",
       " 'I felt that I had let the previous generation of entrepreneurs down, that I had dropped the baton as it was being passed to me.',\n",
       " 'I met with David Packard and Bob Noyce, and tried to apologize for screwing up so badly.',\n",
       " 'I was a very public failure and I even thought about running away from the Valley, but something slowly began to dawn on me.',\n",
       " 'I still loved what I did.',\n",
       " 'Steve Jobs: (07:04) The turn of events at Apple had not changed that one bit.',\n",
       " \"I'd been rejected, but I was still in love.\",\n",
       " 'So I decided to start over.',\n",
       " \"I didn't see it then, but it turned out that getting fired from Apple was the best thing that could've ever happened to me.\",\n",
       " 'The heaviness of being successful was replaced by the lightness of being a beginner again, less sure about everything.',\n",
       " 'It freed me to enter one of the most creative periods of my life.',\n",
       " 'During the next five years, I started a company named Next, another company named Pixar, and fell in love with an amazing woman who would become my wife.',\n",
       " \"Pixar went on to create the world's first computer animated feature film, Toy Story, and is now the most successful animation studio in the world.\",\n",
       " \"Steve Jobs: (07:49) In a remarkable turn of events, Apple bought Next and I returned to Apple, and the technology we developed at Next is at the heart of Apple's current renaissance.\",\n",
       " 'Laurene and I have a wonderful family together.',\n",
       " \"I'm pretty sure none of this would have happened if I hadn't been fired from Apple.\",\n",
       " 'It was awful tasting medicine, but I guess the patient needed it.',\n",
       " \"Sometimes life's going to hit you in the head with a brick.\",\n",
       " \"Don't lose faith.\",\n",
       " \"I'm convinced that the only thing that kept me going was that I loved what I did.\",\n",
       " \"Steve Jobs: (08:21) You've got to find what you love, and that is as true for work as it is for your lovers.\",\n",
       " 'Your work is going to fill a large part of your life, and the only way to be truly satisfied is to do what you believe is great work.',\n",
       " 'The only way to do great work is to love what you do.',\n",
       " \"If you haven't found it yet, keep looking and don't settle.\",\n",
       " \"As with all matters of the heart, you'll know when you find it and, like any great relationship, it just gets better and better as the years roll on.\",\n",
       " 'So keep looking.',\n",
       " \"Don't settle.\",\n",
       " 'Steve Jobs: (09:04) My third story is about death.',\n",
       " 'When I was 17, I read a quote that went something like, \"If you live each day as if it was your last, someday you\\'ll most certainly be right.\"',\n",
       " 'It made an impression on me.',\n",
       " \"Since then, for the past 33 years, I've looked in the mirror every morning and asked myself, if today were the last day of my life, would I want to do what I am about to do today?\",\n",
       " 'Whenever the answer has been no for too many days in a row, I know I need to change something.',\n",
       " \"Remembering that I'll be dead soon is the most important tool I've ever encountered to help me make the big choices in life.\",\n",
       " 'Because almost everything, all external expectations, all pride, all fear of embarrassment or failure, these things just fall away in the face of death, leaving only what is truly important.',\n",
       " 'Remembering that you are going to die is the best way I know to avoid the trap of thinking you have something to lose.',\n",
       " 'You are already naked.',\n",
       " 'There is no reason not to follow your heart.',\n",
       " 'Steve Jobs: (10:11) About a year ago, I was diagnosed with cancer.',\n",
       " 'I had a scan at 7:30 in the morning, and it clearly showed a tumor on my pancreas.',\n",
       " \"I didn't even know what a pancreas was.\",\n",
       " 'The doctors told me this was almost certainly a type of cancer that is incurable and that I should expect to live no longer than three to six months.',\n",
       " \"My doctor advised me to go home and get my affairs in order, which is doctor's code for prepare to die.\",\n",
       " \"It means to try and tell your kids everything you thought you'd have the next 10 years to tell them in just a few months.\",\n",
       " 'It means to make sure everything is buttoned up so that it will be as easy as possible for your family.',\n",
       " 'It means to say your goodbyes.',\n",
       " 'Steve Jobs: (11:00) I lived with that diagnosis all day.',\n",
       " 'Later that evening, I had a biopsy where they stuck an endoscope down my throat, through my stomach, and into my intestines, put a needle into my pancreas and got a few cells from the tumor.',\n",
       " 'I was sedated, but my wife who was there told me that when they viewed the cells under a microscope, the doctor started crying because it turned out to be a very rare form of pancreatic cancer that is curable with surgery.',\n",
       " \"I had the surgery and thankfully, I'm fine now Steve Jobs: (11:40) This was the closest I've been to facing death, and I hope it's the closest I get for a few more decades.\",\n",
       " 'Having lived through it, I can now say this to you with a bit more certainty than when death was a useful, but purely intellectual concept.',\n",
       " 'No one wants to die.',\n",
       " \"Even people who want to go to heaven don't want to die to get there.\",\n",
       " 'Yet, death is the destination we all share.',\n",
       " 'No one has ever escaped it, and that is as it should be because death is very likely the single best invention of life.',\n",
       " \"Steve Jobs: (12:14) It's life's change agent.\",\n",
       " 'It clears out the old to make way for the new.',\n",
       " 'Right now, the new is you, but someday not too long from now, you will gradually become the old and be cleared away.',\n",
       " \"Sorry to be so dramatic, but it's quite true.\",\n",
       " \"Your time is limited, so don't waste it living someone else's life.\",\n",
       " \"Don't be trapped by dogma, which is living with the results of other people's thinking.\",\n",
       " \"Don't let the noise of others opinions drowned out your own inner voice, and most important, have the courage to follow your heart and intuition.\",\n",
       " 'They somehow already know what you truly want to become.',\n",
       " 'Everything else is secondary.',\n",
       " 'Steve Jobs: (13:08) When I was young, there was an amazing publication called the Whole Earth Catalog, which was one of the Bibles of my generation.',\n",
       " 'It was created by a fellow named Stewart Brand, not far from here in Menlo Park, and he brought it to life with his poetic touch.',\n",
       " \"This was in the late 60's before personal computers and desktop publishing.\",\n",
       " 'So it was all made with typewriters, scissors, and Polaroid cameras.',\n",
       " 'It was sort of like Google in paperback form 35 years before Google came along.',\n",
       " 'It was idealistic, overflowing with neat tools, and great notions.',\n",
       " 'Stewart and his team put out several issues of the Whole Earth Catalog, and then when it had run its course, they put out a final issue.',\n",
       " 'Steve Jobs: (13:52) It was the mid 1970s, and I was your age.',\n",
       " 'On the back cover of their final issue was a photograph of an early morning country road, the kind you might find yourself hitchhiking on if you were so adventurous.',\n",
       " 'Beneath it were the words, \"Stay hungry, stay foolish.\"',\n",
       " 'It was their farewell message as they signed off.',\n",
       " 'Stay hungry, stay foolish.',\n",
       " \"I've always wished that for myself, and now as you graduate to begin anew, I wish that for you.\",\n",
       " 'Stay hungry, stay foolish.',\n",
       " 'Thank you all very much.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the POS Tag \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sentences = nltk.sent_tokenize(speech2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thank', 'NNP'), ('.', '.')]\n",
      "[('I', 'PRP'), (\"'m\", 'VBP'), ('honored', 'JJ'), ('today', 'NN'), ('commencement', 'VBD'), ('one', 'CD'), ('finest', 'JJS'), ('universities', 'JJ'), ('world', 'NN'), ('.', '.')]\n",
      "[('Truth', 'NNP'), ('told', 'VBD'), (',', ','), ('I', 'PRP'), ('never', 'RB'), ('graduated', 'VBD'), ('college', 'NN'), (',', ','), ('closest', 'VBP'), ('I', 'PRP'), (\"'ve\", 'VBP'), ('ever', 'RB'), ('gotten', 'VBN'), ('college', 'NN'), ('graduation', 'NN'), ('today', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('want', 'VBP'), ('tell', 'JJ'), ('three', 'CD'), ('stories', 'NNS'), ('life', 'NN'), ('.', '.')]\n",
      "[('That', 'DT'), (\"'s\", 'VBZ'), ('.', '.')]\n",
      "[('No', 'DT'), ('big', 'JJ'), ('deal', 'NN'), ('.', '.')]\n",
      "[('Just', 'RB'), ('three', 'CD'), ('stories', 'NNS'), ('.', '.')]\n",
      "[('The', 'DT'), ('first', 'JJ'), ('story', 'NN'), ('connecting', 'VBG'), ('dots', 'NNS'), ('.', '.')]\n",
      "[('I', 'PRP'), ('dropped', 'VBD'), ('Reed', 'NNP'), ('College', 'NNP'), ('first', 'RB'), ('six', 'CD'), ('months', 'NNS'), (',', ','), ('stayed', 'VBN'), ('around', 'IN'), ('drop-in', 'JJ'), ('another', 'DT'), ('18', 'CD'), ('months', 'NNS'), ('I', 'PRP'), ('really', 'RB'), ('quit', 'VB'), ('.', '.')]\n",
      "[('So', 'RB'), (\"'d\", 'MD'), ('I', 'PRP'), ('drop', 'VB'), ('?', '.')]\n",
      "[('It', 'PRP'), ('started', 'VBD'), ('I', 'PRP'), ('born', 'VBP'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('01:15', 'CD'), (')', ')'), ('My', 'PRP$'), ('biological', 'JJ'), ('mother', 'NN'), ('young', 'JJ'), (',', ','), ('unwed', 'JJ'), ('graduate', 'NN'), ('student', 'NN'), ('decided', 'VBD'), ('put', 'VBN'), ('adoption', 'NN'), ('.', '.')]\n",
      "[('She', 'PRP'), ('felt', 'VBD'), ('strongly', 'RB'), ('I', 'PRP'), ('adopted', 'VBD'), ('college', 'NN'), ('graduates', 'NNS'), ('.', '.')]\n",
      "[('So', 'RB'), ('everything', 'NN'), ('set', 'NN'), ('adopted', 'VBD'), ('birth', 'JJ'), ('lawyer', 'NN'), ('wife', 'NN'), (',', ','), ('except', 'IN'), ('I', 'PRP'), ('popped', 'VBD'), (',', ','), ('decided', 'VBD'), ('last', 'JJ'), ('minute', 'NN'), ('really', 'RB'), ('wanted', 'VBD'), ('girl', 'NN'), ('.', '.')]\n",
      "[('So', 'RB'), ('parents', 'NNS'), (',', ','), ('waiting', 'VBG'), ('list', 'NN'), (',', ','), ('got', 'VBD'), ('call', 'JJ'), ('middle', 'JJ'), ('night', 'NN'), ('asking', 'VBG'), ('``', '``'), ('We', 'PRP'), (\"'ve\", 'VBP'), ('got', 'VBN'), ('unexpected', 'JJ'), ('baby', 'NN'), ('boy', 'NN'), (',', ','), ('want', 'VBP'), ('?', '.'), (\"''\", \"''\")]\n",
      "[('They', 'PRP'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Of', 'IN'), ('course', 'NN'), ('.', '.'), (\"''\", \"''\")]\n",
      "[('My', 'PRP$'), ('biological', 'JJ'), ('mother', 'NN'), ('found', 'VBD'), ('later', 'RB'), ('mother', 'RB'), ('never', 'RB'), ('graduated', 'VBN'), ('college', 'NN'), ('father', 'NN'), ('never', 'RB'), ('graduated', 'VBN'), ('high', 'JJ'), ('school', 'NN'), ('.', '.')]\n",
      "[('She', 'PRP'), ('refused', 'VBD'), ('sign', 'JJ'), ('final', 'JJ'), ('adoption', 'NN'), ('papers', 'NNS'), ('.', '.')]\n",
      "[('She', 'PRP'), ('relented', 'VBD'), ('months', 'NNS'), ('later', 'RB'), ('parents', 'NNS'), ('promised', 'VBD'), ('I', 'PRP'), ('would', 'MD'), ('go', 'VB'), ('college', 'NN'), ('.', '.')]\n",
      "[('This', 'DT'), ('start', 'JJ'), ('life', 'NN'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('02:13', 'CD'), (')', ')'), ('17', 'CD'), ('years', 'NNS'), ('later', 'RB'), (',', ','), ('I', 'PRP'), ('go', 'VBP'), ('college', 'NN'), (',', ','), ('I', 'PRP'), ('naively', 'RB'), ('chose', 'VBD'), ('college', 'NN'), ('almost', 'RB'), ('expensive', 'JJ'), ('Stanford', 'NNP'), (',', ','), ('working', 'VBG'), ('class', 'NN'), ('parents', 'NNS'), ('savings', 'VBP'), ('spent', 'VBN'), ('college', 'NN'), ('tuition', 'NN'), ('.', '.')]\n",
      "[('After', 'IN'), ('six', 'CD'), ('months', 'NNS'), (',', ','), ('I', 'PRP'), ('could', 'MD'), (\"n't\", 'RB'), ('see', 'VB'), ('value', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('idea', 'NN'), ('I', 'PRP'), ('wanted', 'VBD'), ('life', 'NN'), ('idea', 'NN'), ('college', 'NN'), ('going', 'VBG'), ('help', 'JJ'), ('figure', 'NN'), (',', ','), ('I', 'PRP'), ('spending', 'VBG'), ('money', 'NN'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('parents', 'NNS'), ('saved', 'VBN'), ('entire', 'JJ'), ('life', 'NN'), (',', ','), ('I', 'PRP'), ('decided', 'VBD'), ('drop', 'NN'), ('trust', 'NN'), ('would', 'MD'), ('work', 'VB'), ('.', '.')]\n",
      "[('Okay', 'NNP'), ('.', '.')]\n",
      "[('It', 'PRP'), ('pretty', 'RB'), ('scary', 'JJ'), ('time', 'NN'), (',', ','), ('looking', 'VBG'), ('back', 'RB'), (',', ','), ('one', 'CD'), ('best', 'JJS'), ('decisions', 'NNS'), ('I', 'PRP'), ('ever', 'RB'), ('made', 'VBD'), ('.', '.')]\n",
      "[('The', 'DT'), ('minute', 'NN'), ('I', 'PRP'), ('dropped', 'VBD'), (',', ','), ('I', 'PRP'), ('could', 'MD'), ('stop', 'VB'), ('taking', 'VBG'), ('required', 'JJ'), ('classes', 'NNS'), (\"n't\", 'RB'), ('interest', 'NN'), ('begin', 'VB'), ('dropping', 'VBG'), ('ones', 'NNS'), ('looked', 'VBD'), ('far', 'RB'), ('interesting', 'JJ'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('03:04', 'CD'), (')', ')'), ('It', 'PRP'), (\"n't\", 'RB'), ('romantic', 'JJ'), ('.', '.')]\n",
      "[('I', 'PRP'), (\"n't\", 'RB'), ('dorm', 'VB'), ('room', 'NN'), (',', ','), ('I', 'PRP'), ('slept', 'VBP'), ('floor', 'NN'), ('friends', 'NNS'), ('rooms', 'NNS'), ('.', '.')]\n",
      "[('I', 'PRP'), ('returned', 'VBD'), ('Coke', 'NNP'), ('bottles', 'NNS'), ('5', 'CD'), ('cent', 'NN'), ('deposits', 'NNS'), ('buy', 'VBP'), ('food', 'NN'), (',', ','), ('I', 'PRP'), ('would', 'MD'), ('walk', 'VB'), ('seven', 'CD'), ('miles', 'NNS'), ('across', 'IN'), ('town', 'NN'), ('every', 'DT'), ('Sunday', 'NNP'), ('night', 'NN'), ('get', 'VB'), ('one', 'CD'), ('good', 'JJ'), ('meal', 'NN'), ('week', 'NN'), ('Hari', 'NNP'), ('Krishna', 'NNP'), ('temple', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('loved', 'VBD'), (',', ','), ('much', 'JJ'), ('I', 'PRP'), ('stumbled', 'VBD'), ('following', 'VBG'), ('curiosity', 'NN'), ('intuition', 'NN'), ('turned', 'VBD'), ('priceless', 'JJ'), ('later', 'RB'), ('.', '.')]\n",
      "[('Let', 'VB'), ('give', 'VB'), ('one', 'CD'), ('example', 'NN'), (',', ','), ('Reed', 'NNP'), ('College', 'NNP'), ('time', 'NN'), ('offered', 'VBD'), ('perhaps', 'RB'), ('best', 'RBS'), ('calligraphy', 'JJ'), ('instruction', 'NN'), ('country', 'NN'), ('throughout', 'IN'), ('campus', 'NN'), ('.', '.')]\n",
      "[('Every', 'DT'), ('poster', 'NN'), (',', ','), ('every', 'DT'), ('label', 'NN'), ('every', 'DT'), ('drawer', 'NN'), ('beautifully', 'RB'), ('hand', 'NN'), ('calligraphed', 'VBD'), ('.', '.')]\n",
      "[('Because', 'IN'), ('I', 'PRP'), ('dropped', 'VBD'), (\"n't\", 'RB'), ('take', 'VB'), ('normal', 'JJ'), ('classes', 'NNS'), (',', ','), ('I', 'PRP'), ('decided', 'VBD'), ('take', 'VB'), ('calligraphy', 'NN'), ('class', 'NN'), ('learn', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('learned', 'VBD'), ('serif', 'JJ'), ('sanserif', 'NN'), ('typefaces', 'NNS'), (',', ','), ('varying', 'VBG'), ('amount', 'NN'), ('space', 'NN'), ('different', 'JJ'), ('letter', 'NN'), ('combinations', 'NNS'), (',', ','), ('makes', 'VBZ'), ('great', 'JJ'), ('typography', 'JJ'), ('great', 'JJ'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('04:03', 'CD'), (')', ')'), ('It', 'PRP'), ('beautiful', 'JJ'), (',', ','), ('historical', 'JJ'), (',', ','), ('artistically', 'RB'), ('subtle', 'JJ'), ('way', 'NN'), ('science', 'NN'), ('ca', 'MD'), (\"n't\", 'RB'), ('capture', 'VB'), (',', ','), ('I', 'PRP'), ('found', 'VBD'), ('fascinating', 'VBG'), ('.', '.')]\n",
      "[('None', 'NN'), ('even', 'RB'), ('hope', 'VBP'), ('practical', 'JJ'), ('application', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('But', 'CC'), ('10', 'CD'), ('years', 'NNS'), ('later', 'RB'), (',', ','), ('designing', 'VBG'), ('first', 'RB'), ('Macintosh', 'NNP'), ('computer', 'NN'), (',', ','), ('came', 'VBD'), ('back', 'RB'), ('designed', 'VBN'), ('Mac', 'NNP'), ('.', '.')]\n",
      "[('It', 'PRP'), ('first', 'RB'), ('computer', 'NN'), ('beautiful', 'JJ'), ('typography', 'NN'), ('.', '.')]\n",
      "[('If', 'IN'), ('I', 'PRP'), ('never', 'RB'), ('dropped', 'VBD'), ('single', 'JJ'), ('course', 'NN'), ('college', 'NN'), (',', ','), ('Mac', 'NNP'), ('would', 'MD'), ('never', 'RB'), ('multiple', 'VB'), ('typefaces', 'NNS'), ('proportionally', 'RB'), ('spaced', 'VBD'), ('fonts', 'NNS'), ('.', '.')]\n",
      "[('Since', 'IN'), ('Windows', 'NNP'), ('copied', 'VBD'), ('Mac', 'NNP'), (',', ','), (\"'s\", 'VBZ'), ('likely', 'JJ'), ('personal', 'JJ'), ('computer', 'NN'), ('would', 'MD'), ('.', '.')]\n",
      "[('If', 'IN'), ('I', 'PRP'), ('never', 'RB'), ('dropped', 'VBD'), (',', ','), ('I', 'PRP'), ('would', 'MD'), ('never', 'RB'), ('dropped', 'VB'), ('calligraphy', 'NN'), ('class', 'NN'), ('personal', 'JJ'), ('computers', 'NNS'), ('might', 'MD'), ('wonderful', 'VB'), ('typography', 'NN'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('04:58', 'CD'), (')', ')'), ('Of', 'IN'), ('course', 'NN'), (',', ','), ('impossible', 'JJ'), ('connect', 'NN'), ('dots', 'NNS'), ('looking', 'VBG'), ('forward', 'RB'), ('I', 'PRP'), ('college', 'NN'), (',', ','), (',', ','), ('clear', 'JJ'), ('looking', 'VBG'), ('backwards', 'NNS'), ('10', 'CD'), ('years', 'NNS'), ('later', 'RB'), ('.', '.')]\n",
      "[('Again', 'RB'), (',', ','), ('ca', 'MD'), (\"n't\", 'RB'), ('connect', 'VB'), ('dots', 'NNS'), ('looking', 'VBG'), ('forward', 'RB'), ('.', '.')]\n",
      "[('You', 'PRP'), ('connect', 'VBP'), ('looking', 'VBG'), ('backwards', 'NNS'), ('.', '.')]\n",
      "[('So', 'RB'), ('trust', 'JJ'), ('dots', 'NNS'), ('somehow', 'VBP'), ('connect', 'JJ'), ('future', 'NN'), ('.', '.')]\n",
      "[('You', 'PRP'), ('trust', 'VBP'), ('something', 'NN'), (',', ','), ('gut', 'NN'), (',', ','), ('destiny', 'NN'), (',', ','), ('life', 'NN'), (',', ','), ('karma', 'NN'), (',', ','), ('whatever', 'WDT'), (',', ','), ('believing', 'VBG'), ('dots', 'NNS'), ('connect', 'VBP'), ('road', 'NN'), ('give', 'VBP'), ('confidence', 'NN'), ('follow', 'JJ'), ('heart', 'NN'), (',', ','), ('even', 'RB'), ('leads', 'VBZ'), ('well-worn', 'JJ'), ('path', 'NN'), (',', ','), ('make', 'VBP'), ('difference', 'NN'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('05:38', 'CD'), (')', ')'), ('My', 'PRP$'), ('second', 'JJ'), ('story', 'NN'), ('love', 'NN'), ('loss', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('lucky', 'VBP'), ('.', '.')]\n",
      "[('I', 'PRP'), ('found', 'VBD'), ('I', 'PRP'), ('love', 'VBP'), ('early', 'JJ'), ('life', 'NN'), ('.', '.')]\n",
      "[('Woz', 'NNP'), ('I', 'PRP'), ('started', 'VBD'), ('Apple', 'NNP'), ('parents', 'NNS'), (\"'\", 'POS'), ('garage', 'NN'), ('I', 'PRP'), ('20', 'CD'), (',', ','), ('worked', 'VBD'), ('hard', 'JJ'), ('10', 'CD'), ('years', 'NNS'), (',', ','), ('Apple', 'NNP'), ('grown', 'VBP'), ('two', 'CD'), ('us', 'PRP'), ('garage', 'NN'), (',', ','), ('2', 'CD'), ('billion', 'CD'), ('company', 'NN'), ('4,000', 'CD'), ('employees', 'NNS'), ('.', '.')]\n",
      "[('We', 'PRP'), ('released', 'VBD'), ('finest', 'JJS'), ('creation', 'NN'), (',', ','), ('Macintosh', 'NNP'), (',', ','), ('year', 'NN'), ('earlier', 'RBR'), (',', ','), ('I', 'PRP'), ('turned', 'VBD'), ('30', 'CD'), (',', ','), ('I', 'PRP'), ('got', 'VBD'), ('fired', 'VBN'), ('.', '.')]\n",
      "[('How', 'WRB'), ('get', 'VB'), ('fired', 'VBN'), ('company', 'NN'), ('started', 'VBD'), ('?', '.')]\n",
      "[('Well', 'RB'), (',', ','), ('Apple', 'NNP'), ('grew', 'VBD'), (',', ','), ('hired', 'VBD'), ('someone', 'NN'), ('I', 'PRP'), ('thought', 'VBD'), ('talented', 'VBN'), ('run', 'NN'), ('company', 'NN'), (',', ','), ('first', 'JJ'), ('year', 'NN'), (',', ','), ('things', 'NNS'), ('went', 'VBD'), ('well', 'RB'), ('.', '.')]\n",
      "[('But', 'CC'), ('visions', 'NNS'), ('future', 'VBP'), ('began', 'VBD'), ('diverge', 'JJ'), (',', ','), ('eventually', 'RB'), ('falling', 'VBG'), ('.', '.')]\n",
      "[('When', 'WRB'), (',', ','), ('board', 'NN'), ('directors', 'NNS'), ('sided', 'VBD'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('06:28', 'CD'), (')', ')'), ('So', 'RB'), ('30', 'CD'), (',', ','), ('I', 'PRP'), ('publicly', 'RB'), ('.', '.')]\n",
      "[('What', 'WP'), ('focus', 'NN'), ('entire', 'JJ'), ('adult', 'NN'), ('life', 'NN'), ('gone', 'VBN'), ('devastating', 'JJ'), ('.', '.')]\n",
      "[('I', 'PRP'), ('really', 'RB'), (\"n't\", 'RB'), ('know', 'VB'), ('months', 'NNS'), ('.', '.')]\n",
      "[('I', 'PRP'), ('felt', 'VBD'), ('I', 'PRP'), ('let', 'VBP'), ('previous', 'JJ'), ('generation', 'NN'), ('entrepreneurs', 'NNS'), (',', ','), ('I', 'PRP'), ('dropped', 'VBD'), ('baton', 'NN'), ('passed', 'VBN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('met', 'VBD'), ('David', 'NNP'), ('Packard', 'NNP'), ('Bob', 'NNP'), ('Noyce', 'NNP'), (',', ','), ('tried', 'VBD'), ('apologize', 'JJ'), ('screwing', 'VBG'), ('badly', 'RB'), ('.', '.')]\n",
      "[('I', 'PRP'), ('public', 'JJ'), ('failure', 'NN'), ('I', 'PRP'), ('even', 'RB'), ('thought', 'VBD'), ('running', 'VBG'), ('away', 'RB'), ('Valley', 'NNP'), (',', ','), ('something', 'NN'), ('slowly', 'RB'), ('began', 'VBD'), ('dawn', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('still', 'RB'), ('loved', 'VBD'), ('I', 'PRP'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('07:04', 'CD'), (')', ')'), ('The', 'DT'), ('turn', 'NN'), ('events', 'NNS'), ('Apple', 'NNP'), ('changed', 'VBD'), ('one', 'CD'), ('bit', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), (\"'d\", 'MD'), ('rejected', 'VBN'), (',', ','), ('I', 'PRP'), ('still', 'RB'), ('love', 'VBP'), ('.', '.')]\n",
      "[('So', 'RB'), ('I', 'PRP'), ('decided', 'VBD'), ('start', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), (\"n't\", 'RB'), ('see', 'VB'), (',', ','), ('turned', 'VBD'), ('getting', 'VBG'), ('fired', 'JJ'), ('Apple', 'NNP'), ('best', 'JJS'), ('thing', 'NN'), ('could', 'MD'), (\"'ve\", 'VBP'), ('ever', 'RB'), ('happened', 'VBN'), ('.', '.')]\n",
      "[('The', 'DT'), ('heaviness', 'NN'), ('successful', 'JJ'), ('replaced', 'VBD'), ('lightness', 'JJ'), ('beginner', 'NN'), (',', ','), ('less', 'JJR'), ('sure', 'JJ'), ('everything', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('freed', 'VBD'), ('enter', 'RBR'), ('one', 'CD'), ('creative', 'NN'), ('periods', 'NNS'), ('life', 'NN'), ('.', '.')]\n",
      "[('During', 'IN'), ('next', 'JJ'), ('five', 'CD'), ('years', 'NNS'), (',', ','), ('I', 'PRP'), ('started', 'VBD'), ('company', 'NN'), ('named', 'VBN'), ('Next', 'NNP'), (',', ','), ('another', 'DT'), ('company', 'NN'), ('named', 'VBN'), ('Pixar', 'NNP'), (',', ','), ('fell', 'VBD'), ('love', 'IN'), ('amazing', 'VBG'), ('woman', 'NN'), ('would', 'MD'), ('become', 'VB'), ('wife', 'NN'), ('.', '.')]\n",
      "[('Pixar', 'NNP'), ('went', 'VBD'), ('create', 'JJ'), ('world', 'NN'), (\"'s\", 'POS'), ('first', 'JJ'), ('computer', 'NN'), ('animated', 'VBN'), ('feature', 'NN'), ('film', 'NN'), (',', ','), ('Toy', 'NNP'), ('Story', 'NNP'), (',', ','), ('successful', 'JJ'), ('animation', 'NN'), ('studio', 'NN'), ('world', 'NN'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('07:49', 'CD'), (')', ')'), ('In', 'IN'), ('remarkable', 'JJ'), ('turn', 'NN'), ('events', 'NNS'), (',', ','), ('Apple', 'NNP'), ('bought', 'VBD'), ('Next', 'NNP'), ('I', 'PRP'), ('returned', 'VBD'), ('Apple', 'NNP'), (',', ','), ('technology', 'NN'), ('developed', 'VBD'), ('Next', 'JJ'), ('heart', 'NN'), ('Apple', 'NNP'), (\"'s\", 'POS'), ('current', 'JJ'), ('renaissance', 'NN'), ('.', '.')]\n",
      "[('Laurene', 'NNP'), ('I', 'PRP'), ('wonderful', 'VBP'), ('family', 'NN'), ('together', 'RB'), ('.', '.')]\n",
      "[('I', 'PRP'), (\"'m\", 'VBP'), ('pretty', 'JJ'), ('sure', 'JJ'), ('none', 'NN'), ('would', 'MD'), ('happened', 'VB'), ('I', 'PRP'), (\"n't\", 'RB'), ('fired', 'VBD'), ('Apple', 'NNP'), ('.', '.')]\n",
      "[('It', 'PRP'), ('awful', 'JJ'), ('tasting', 'VBG'), ('medicine', 'NN'), (',', ','), ('I', 'PRP'), ('guess', 'VBP'), ('patient', 'JJ'), ('needed', 'VBN'), ('.', '.')]\n",
      "[('Sometimes', 'RB'), ('life', 'NN'), (\"'s\", 'POS'), ('going', 'VBG'), ('hit', 'JJ'), ('head', 'NN'), ('brick', 'NN'), ('.', '.')]\n",
      "[('Do', 'VBP'), (\"n't\", 'RB'), ('lose', 'VB'), ('faith', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), (\"'m\", 'VBP'), ('convinced', 'JJ'), ('thing', 'NN'), ('kept', 'VBD'), ('going', 'VBG'), ('I', 'PRP'), ('loved', 'VBD'), ('I', 'PRP'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('08:21', 'CD'), (')', ')'), ('You', 'PRP'), (\"'ve\", 'VBP'), ('got', 'VBN'), ('find', 'RB'), ('love', 'JJ'), (',', ','), ('true', 'JJ'), ('work', 'NN'), ('lovers', 'NNS'), ('.', '.')]\n",
      "[('Your', 'PRP$'), ('work', 'NN'), ('going', 'VBG'), ('fill', 'JJ'), ('large', 'JJ'), ('part', 'NN'), ('life', 'NN'), (',', ','), ('way', 'NN'), ('truly', 'RB'), ('satisfied', 'JJ'), ('believe', 'VBP'), ('great', 'JJ'), ('work', 'NN'), ('.', '.')]\n",
      "[('The', 'DT'), ('way', 'NN'), ('great', 'JJ'), ('work', 'NN'), ('love', 'NN'), ('.', '.')]\n",
      "[('If', 'IN'), (\"n't\", 'RB'), ('found', 'VBN'), ('yet', 'RB'), (',', ','), ('keep', 'VB'), ('looking', 'VBG'), (\"n't\", 'RB'), ('settle', 'VB'), ('.', '.')]\n",
      "[('As', 'IN'), ('matters', 'NNS'), ('heart', 'NN'), (',', ','), (\"'ll\", 'MD'), ('know', 'VB'), ('find', 'RB'), (',', ','), ('like', 'IN'), ('great', 'JJ'), ('relationship', 'NN'), (',', ','), ('gets', 'VBZ'), ('better', 'JJR'), ('better', 'JJR'), ('years', 'NNS'), ('roll', 'VBP'), ('.', '.')]\n",
      "[('So', 'RB'), ('keep', 'JJ'), ('looking', 'VBG'), ('.', '.')]\n",
      "[('Do', 'VBP'), (\"n't\", 'RB'), ('settle', 'VB'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('09:04', 'CD'), (')', ')'), ('My', 'PRP$'), ('third', 'JJ'), ('story', 'NN'), ('death', 'NN'), ('.', '.')]\n",
      "[('When', 'WRB'), ('I', 'PRP'), ('17', 'CD'), (',', ','), ('I', 'PRP'), ('read', 'VBP'), ('quote', 'JJ'), ('went', 'VBD'), ('something', 'NN'), ('like', 'IN'), (',', ','), ('``', '``'), ('If', 'IN'), ('live', 'JJ'), ('day', 'NN'), ('last', 'JJ'), (',', ','), ('someday', 'JJ'), (\"'ll\", 'MD'), ('certainly', 'RB'), ('right', 'RB'), ('.', '.'), (\"''\", \"''\")]\n",
      "[('It', 'PRP'), ('made', 'VBD'), ('impression', 'NN'), ('.', '.')]\n",
      "[('Since', 'IN'), (',', ','), ('past', 'IN'), ('33', 'CD'), ('years', 'NNS'), (',', ','), ('I', 'PRP'), (\"'ve\", 'VBP'), ('looked', 'VBN'), ('mirror', 'NN'), ('every', 'DT'), ('morning', 'NN'), ('asked', 'VBN'), (',', ','), ('today', 'NN'), ('last', 'JJ'), ('day', 'NN'), ('life', 'NN'), (',', ','), ('would', 'MD'), ('I', 'PRP'), ('want', 'VBP'), ('I', 'PRP'), ('today', 'NN'), ('?', '.')]\n",
      "[('Whenever', 'WRB'), ('answer', 'RB'), ('many', 'JJ'), ('days', 'NNS'), ('row', 'VBP'), (',', ','), ('I', 'PRP'), ('know', 'VBP'), ('I', 'PRP'), ('need', 'VBP'), ('change', 'JJ'), ('something', 'NN'), ('.', '.')]\n",
      "[('Remembering', 'VBG'), ('I', 'PRP'), (\"'ll\", 'MD'), ('dead', 'VB'), ('soon', 'RB'), ('important', 'JJ'), ('tool', 'NN'), ('I', 'PRP'), (\"'ve\", 'VBP'), ('ever', 'RB'), ('encountered', 'VBN'), ('help', 'NN'), ('make', 'VB'), ('big', 'JJ'), ('choices', 'NNS'), ('life', 'NN'), ('.', '.')]\n",
      "[('Because', 'IN'), ('almost', 'RB'), ('everything', 'NN'), (',', ','), ('external', 'JJ'), ('expectations', 'NNS'), (',', ','), ('pride', 'NN'), (',', ','), ('fear', 'JJ'), ('embarrassment', 'NN'), ('failure', 'NN'), (',', ','), ('things', 'NNS'), ('fall', 'VBP'), ('away', 'RB'), ('face', 'JJ'), ('death', 'NN'), (',', ','), ('leaving', 'VBG'), ('truly', 'RB'), ('important', 'JJ'), ('.', '.')]\n",
      "[('Remembering', 'VBG'), ('going', 'VBG'), ('die', 'JJ'), ('best', 'JJS'), ('way', 'NN'), ('I', 'PRP'), ('know', 'VBP'), ('avoid', 'JJ'), ('trap', 'NN'), ('thinking', 'VBG'), ('something', 'NN'), ('lose', 'NN'), ('.', '.')]\n",
      "[('You', 'PRP'), ('already', 'RB'), ('naked', 'VBD'), ('.', '.')]\n",
      "[('There', 'EX'), ('reason', 'NN'), ('follow', 'JJ'), ('heart', 'NN'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('10:11', 'CD'), (')', ')'), ('About', 'IN'), ('year', 'NN'), ('ago', 'RB'), (',', ','), ('I', 'PRP'), ('diagnosed', 'VBD'), ('cancer', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('scan', 'VBP'), ('7:30', 'CD'), ('morning', 'NN'), (',', ','), ('clearly', 'RB'), ('showed', 'VBD'), ('tumor', 'JJ'), ('pancreas', 'NNS'), ('.', '.')]\n",
      "[('I', 'PRP'), (\"n't\", 'RB'), ('even', 'RB'), ('know', 'VB'), ('pancreas', 'NNS'), ('.', '.')]\n",
      "[('The', 'DT'), ('doctors', 'NNS'), ('told', 'VBD'), ('almost', 'RB'), ('certainly', 'RB'), ('type', 'JJ'), ('cancer', 'NN'), ('incurable', 'JJ'), ('I', 'PRP'), ('expect', 'VBP'), ('live', 'JJ'), ('longer', 'JJR'), ('three', 'CD'), ('six', 'CD'), ('months', 'NNS'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('doctor', 'NN'), ('advised', 'VBD'), ('go', 'VB'), ('home', 'NN'), ('get', 'NN'), ('affairs', 'NNS'), ('order', 'NN'), (',', ','), ('doctor', 'NN'), (\"'s\", 'POS'), ('code', 'NN'), ('prepare', 'NN'), ('die', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('means', 'VBZ'), ('try', 'VB'), ('tell', 'VB'), ('kids', 'NNS'), ('everything', 'NN'), ('thought', 'VBD'), (\"'d\", 'MD'), ('next', 'JJ'), ('10', 'CD'), ('years', 'NNS'), ('tell', 'JJ'), ('months', 'NNS'), ('.', '.')]\n",
      "[('It', 'PRP'), ('means', 'VBZ'), ('make', 'JJ'), ('sure', 'JJ'), ('everything', 'NN'), ('buttoned', 'VBD'), ('easy', 'JJ'), ('possible', 'JJ'), ('family', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('means', 'VBZ'), ('say', 'JJ'), ('goodbyes', 'NNS'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('11:00', 'CD'), (')', ')'), ('I', 'PRP'), ('lived', 'VBD'), ('diagnosis', 'NN'), ('day', 'NN'), ('.', '.')]\n",
      "[('Later', 'RB'), ('evening', 'NN'), (',', ','), ('I', 'PRP'), ('biopsy', 'VBP'), ('stuck', 'JJ'), ('endoscope', 'NN'), ('throat', 'NN'), (',', ','), ('stomach', 'NN'), (',', ','), ('intestines', 'NNS'), (',', ','), ('put', 'VBD'), ('needle', 'JJ'), ('pancreas', 'NNS'), ('got', 'VBD'), ('cells', 'NNS'), ('tumor', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('sedated', 'VBD'), (',', ','), ('wife', 'NN'), ('told', 'VBD'), ('viewed', 'VBN'), ('cells', 'NNS'), ('microscope', 'NN'), (',', ','), ('doctor', 'NN'), ('started', 'VBD'), ('crying', 'VBG'), ('turned', 'VBN'), ('rare', 'JJ'), ('form', 'NN'), ('pancreatic', 'JJ'), ('cancer', 'NN'), ('curable', 'JJ'), ('surgery', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('surgery', 'VBP'), ('thankfully', 'RB'), (',', ','), ('I', 'PRP'), (\"'m\", 'VBP'), ('fine', 'JJ'), ('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('11:40', 'CD'), (')', ')'), ('This', 'DT'), ('closest', 'JJS'), ('I', 'PRP'), (\"'ve\", 'VBP'), ('facing', 'VBG'), ('death', 'NN'), (',', ','), ('I', 'PRP'), ('hope', 'VBP'), (\"'s\", 'POS'), ('closest', 'JJS'), ('I', 'PRP'), ('get', 'VBP'), ('decades', 'NNS'), ('.', '.')]\n",
      "[('Having', 'VBG'), ('lived', 'VBN'), (',', ','), ('I', 'PRP'), ('say', 'VBP'), ('bit', 'JJ'), ('certainty', 'JJ'), ('death', 'NN'), ('useful', 'JJ'), (',', ','), ('purely', 'RB'), ('intellectual', 'JJ'), ('concept', 'NN'), ('.', '.')]\n",
      "[('No', 'DT'), ('one', 'NN'), ('wants', 'VBZ'), ('die', 'NN'), ('.', '.')]\n",
      "[('Even', 'RB'), ('people', 'NNS'), ('want', 'VBP'), ('go', 'VB'), ('heaven', 'VBP'), (\"n't\", 'RB'), ('want', 'VB'), ('die', 'JJ'), ('get', 'NN'), ('.', '.')]\n",
      "[('Yet', 'RB'), (',', ','), ('death', 'NN'), ('destination', 'NN'), ('share', 'NN'), ('.', '.')]\n",
      "[('No', 'DT'), ('one', 'NN'), ('ever', 'RB'), ('escaped', 'VBD'), (',', ','), ('death', 'NN'), ('likely', 'JJ'), ('single', 'JJ'), ('best', 'JJS'), ('invention', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('12:14', 'CD'), (')', ')'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('life', 'NN'), (\"'s\", 'POS'), ('change', 'NN'), ('agent', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('clears', 'VBZ'), ('old', 'JJ'), ('make', 'VB'), ('way', 'NN'), ('new', 'JJ'), ('.', '.')]\n",
      "[('Right', 'RB'), (',', ','), ('new', 'JJ'), (',', ','), ('someday', 'JJ'), ('long', 'RB'), (',', ','), ('gradually', 'RB'), ('become', 'VBN'), ('old', 'JJ'), ('cleared', 'JJ'), ('away', 'RB'), ('.', '.')]\n",
      "[('Sorry', 'NNP'), ('dramatic', 'JJ'), (',', ','), (\"'s\", 'POS'), ('quite', 'RB'), ('true', 'JJ'), ('.', '.')]\n",
      "[('Your', 'PRP$'), ('time', 'NN'), ('limited', 'JJ'), (',', ','), (\"n't\", 'RB'), ('waste', 'VBD'), ('living', 'VBG'), ('someone', 'NN'), ('else', 'RB'), (\"'s\", 'POS'), ('life', 'NN'), ('.', '.')]\n",
      "[('Do', 'VBP'), (\"n't\", 'RB'), ('trapped', 'VB'), ('dogma', 'NN'), (',', ','), ('living', 'JJ'), ('results', 'NNS'), ('people', 'NNS'), (\"'s\", 'POS'), ('thinking', 'NN'), ('.', '.')]\n",
      "[('Do', 'VBP'), (\"n't\", 'RB'), ('let', 'VB'), ('noise', 'VB'), ('others', 'NNS'), ('opinions', 'NNS'), ('drowned', 'VBD'), ('inner', 'JJ'), ('voice', 'NN'), (',', ','), ('important', 'JJ'), (',', ','), ('courage', 'NN'), ('follow', 'JJ'), ('heart', 'NN'), ('intuition', 'NN'), ('.', '.')]\n",
      "[('They', 'PRP'), ('somehow', 'VBP'), ('already', 'RB'), ('know', 'VBP'), ('truly', 'RB'), ('want', 'JJ'), ('become', 'NN'), ('.', '.')]\n",
      "[('Everything', 'NN'), ('else', 'RB'), ('secondary', 'JJ'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('13:08', 'CD'), (')', ')'), ('When', 'WRB'), ('I', 'PRP'), ('young', 'JJ'), (',', ','), ('amazing', 'JJ'), ('publication', 'NN'), ('called', 'VBN'), ('Whole', 'NNP'), ('Earth', 'NNP'), ('Catalog', 'NNP'), (',', ','), ('one', 'CD'), ('Bibles', 'NNP'), ('generation', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('created', 'VBD'), ('fellow', 'NN'), ('named', 'VBN'), ('Stewart', 'NNP'), ('Brand', 'NNP'), (',', ','), ('far', 'RB'), ('Menlo', 'NNP'), ('Park', 'NNP'), (',', ','), ('brought', 'VBD'), ('life', 'NN'), ('poetic', 'JJ'), ('touch', 'NN'), ('.', '.')]\n",
      "[('This', 'DT'), ('late', 'JJ'), ('60', 'CD'), (\"'s\", 'POS'), ('personal', 'JJ'), ('computers', 'NNS'), ('desktop', 'VBP'), ('publishing', 'NN'), ('.', '.')]\n",
      "[('So', 'RB'), ('made', 'JJ'), ('typewriters', 'NNS'), (',', ','), ('scissors', 'NNS'), (',', ','), ('Polaroid', 'NNP'), ('cameras', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('sort', 'VBZ'), ('like', 'IN'), ('Google', 'NNP'), ('paperback', 'VBP'), ('form', 'NN'), ('35', 'CD'), ('years', 'NNS'), ('Google', 'NNP'), ('came', 'VBD'), ('along', 'RB'), ('.', '.')]\n",
      "[('It', 'PRP'), ('idealistic', 'JJ'), (',', ','), ('overflowing', 'VBG'), ('neat', 'NN'), ('tools', 'NNS'), (',', ','), ('great', 'JJ'), ('notions', 'NNS'), ('.', '.')]\n",
      "[('Stewart', 'NNP'), ('team', 'NN'), ('put', 'VBD'), ('several', 'JJ'), ('issues', 'NNS'), ('Whole', 'NNP'), ('Earth', 'NNP'), ('Catalog', 'NNP'), (',', ','), ('run', 'VB'), ('course', 'NN'), (',', ','), ('put', 'VBD'), ('final', 'JJ'), ('issue', 'NN'), ('.', '.')]\n",
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), (':', ':'), ('(', '('), ('13:52', 'CD'), (')', ')'), ('It', 'PRP'), ('mid', 'VBZ'), ('1970s', 'CD'), (',', ','), ('I', 'PRP'), ('age', 'NN'), ('.', '.')]\n",
      "[('On', 'IN'), ('back', 'RB'), ('cover', 'JJ'), ('final', 'JJ'), ('issue', 'NN'), ('photograph', 'VBD'), ('early', 'JJ'), ('morning', 'NN'), ('country', 'NN'), ('road', 'NN'), (',', ','), ('kind', 'NN'), ('might', 'MD'), ('find', 'VB'), ('hitchhiking', 'VBG'), ('adventurous', 'JJ'), ('.', '.')]\n",
      "[('Beneath', 'NNP'), ('words', 'NNS'), (',', ','), ('``', '``'), ('Stay', 'NNP'), ('hungry', 'JJ'), (',', ','), ('stay', 'JJ'), ('foolish', 'JJ'), ('.', '.'), (\"''\", \"''\")]\n",
      "[('It', 'PRP'), ('farewell', 'RB'), ('message', 'NN'), ('signed', 'VBD'), ('.', '.')]\n",
      "[('Stay', 'NNP'), ('hungry', 'JJ'), (',', ','), ('stay', 'JJ'), ('foolish', 'JJ'), ('.', '.')]\n",
      "[('I', 'PRP'), (\"'ve\", 'VBP'), ('always', 'RB'), ('wished', 'VBN'), (',', ','), ('graduate', 'JJ'), ('begin', 'NN'), ('anew', 'RB'), (',', ','), ('I', 'PRP'), ('wish', 'VBP'), ('.', '.')]\n",
      "[('Stay', 'NNP'), ('hungry', 'JJ'), (',', ','), ('stay', 'JJ'), ('foolish', 'JJ'), ('.', '.')]\n",
      "[('Thank', 'RB'), ('much', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [word for word in words if word not in set(stopwords.words('english'))]\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    print(pos_tag)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of Speech legena\n",
    "# CC coordinating conjunction\n",
    "# CD cardinal digit\n",
    "# DT determiner\n",
    "# EX existential there (like: \"there has\")\n",
    "# FW foreign word\n",
    "# IN preposition/subordinating conjunction\n",
    "# JJ adjective 'big'\n",
    "# JJR adjective, comparative 'bigger'\n",
    "# JJS adjective, superlative 'biggest'\n",
    "# LS list marker 1)\n",
    "# MD modal\n",
    "# NN noun, singular 'desk'\n",
    "# NNS noun plural 'desks'\n",
    "# NNP proper noun, singular 'Harrison'\n",
    "# NNPS proper noun, plural 'Smiths'\n",
    "# PDT predeterminer $a$\n",
    "# POS possessive ending parent's\n",
    "# PRP personal pronoun\n",
    "# PRP$ possessive pronoun\n",
    "# RB adverb\n",
    "# RBR adverb, comparative better\n",
    "# RBS adverb, superlative best\n",
    "# RP abbreviation \"Dr.\"\n",
    "# TO to\n",
    "# UH interjection\n",
    "# VB verb, base form take\n",
    "# VBD verb, past tense took\n",
    "# VBG verb, gerund/present participle taking\n",
    "# VBN verb, past participle taken\n",
    "# VBP verb, sing. present, non-3d take\n",
    "# VBZ verb, 3rd person sing. present takes\n",
    "# WDT wh-determiner which\n",
    "# WP wh-pronoun who, what\n",
    "# WP$ possessive wh-pronoun whose\n",
    "# WRB wh-adverb how\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Taj', 'NNP'),\n",
       " ('Mahal', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('beautiful', 'JJ'),\n",
       " ('monument', 'NN')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(\"Taj Mahal is a beautiful monument\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Taj', 'NNP'), ('NNP', 'NNP')]\n",
      "[('Mahal', 'NNP'), ('NNP', 'NNP')]\n",
      "[('is', 'VBZ'), ('VBZ', 'NNP')]\n",
      "[('a', 'DT'), ('DT', 'NNP')]\n",
      "[('beautiful', 'JJ'), ('JJ', 'NNP')]\n",
      "[('monument', 'NN'), ('NN', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "for word in nltk.pos_tag(\"Taj Mahal is a beautiful monument\".split()):\n",
    "    print(nltk.pos_tag(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Entity Recognition (NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('Eiffel', 'NNP'), ('Tower', 'NNP'), ('was', 'VBD'), ('built', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('World', 'NNP'), ('Exhibition', 'NNP'), ('in', 'IN'), ('1889', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The Eiffel Tower was built for the World Exhibition in 1889\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "tag_elements = nltk.pos_tag(words)\n",
    "print(tag_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('Eiffel', 'NNP'),\n",
       " ('Tower', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('built', 'VBN'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('World', 'NNP'),\n",
       " ('Exhibition', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('1889', 'CD')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The Eiffel Tower was built for the World Exhibition in 1889\"\n",
    "nltk.pos_tag(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: svgling in /Users/jasper/Desktop/Introduction_AIML/venv/lib/python3.12/site-packages (0.5.0)\n",
      "Requirement already satisfied: svgwrite in /Users/jasper/Desktop/Introduction_AIML/venv/lib/python3.12/site-packages (from svgling) (1.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/jasper/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/jasper/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install svgling\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,568.0,168.0\" width=\"568px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"7.04225%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.52113%\" y1=\"20px\" y2=\"48px\" /><svg width=\"21.1268%\" x=\"7.04225%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"53.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"46.6667%\" x=\"53.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Tower</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6667%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.6056%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.04225%\" x=\"28.169%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.6901%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.85915%\" x=\"35.2113%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">built</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.1408%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.04225%\" x=\"45.0704%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">for</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"48.5915%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.04225%\" x=\"52.1127%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.6338%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.85915%\" x=\"59.1549%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">World</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.0845%\" y1=\"20px\" y2=\"48px\" /><svg width=\"16.9014%\" x=\"69.0141%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Exhibition</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.4648%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.6338%\" x=\"85.9155%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.7324%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.4507%\" x=\"91.5493%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1889</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.7746%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Eiffel', 'NNP'), ('Tower', 'NNP')]), ('was', 'VBD'), ('built', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('World', 'NNP'), ('Exhibition', 'NNP'), ('in', 'IN'), ('1889', 'CD')])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.ne_chunk(tag_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Converting words into vectors:\n",
    "\n",
    "1: One Hot Encoding\n",
    "\n",
    "2: Bag of Words\n",
    "\n",
    "3: TF-IDF\n",
    "\n",
    "4: Word 2 Vec\n",
    "\n",
    "5: Avage Word 2 Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is a widely used technique in machine learning and data preprocessing that transforms categorical variables into a numerical format suitable for algorithmic processing. This method is essential because many machine learning algorithms require numerical input to function effectively, as they cannot directly handle categorical data, which consists of label values rather than numeric values.\n",
    "\n",
    "How One-Hot Encoding Works:\n",
    "\n",
    "The process of one-hot encoding involves converting each unique category in a categorical variable into a new binary column. Each column corresponds to a category, and the presence of a category in an observation is indicated by a value of 1, while the absence is marked by 0. For example, if we have a categorical feature \"Color\" with three possible values—Red, Green, and Blue—one-hot encoding would create three new binary columns: \"Color_Red,\" \"Color_Green,\" and \"Color_Blue.\" An observation with the color Red would be represented as [1, 0, 0], indicating that it belongs to the Red category.\n",
    "\n",
    "Advantages of One-Hot Encoding:\n",
    "\n",
    "One-hot encoding has several advantages:\n",
    "1: Preservation of Information: It maintains the distinctiveness of each category without imposing any ordinal relationship that could mislead machine learning models.\n",
    "2: Compatibility: It ensures that datasets are compatible with various machine learning algorithms that expect numerical input.\n",
    "3: Intuitiveness: The resulting binary representation is straightforward and easy to understand.\n",
    "\n",
    "Disadvantages of One-Hot Encoding\n",
    "However, one-hot encoding also has its drawbacks:\n",
    "1: Dimensionality Increase: It can significantly increase the dimensionality of the dataset, especially when dealing with categorical variables that have many categories, potentially leading to the \"curse of dimensionality.\"\n",
    "2: Sparse Matrix: The resulting sparse matrix can be computationally intensive for some models to handle.\n",
    "3: Information Loss: It does not capture any ordinal relationships if they exist within the categorical variable.\n",
    "\n",
    "When to Use One-Hot Encoding:\n",
    "\n",
    "One-hot encoding is particularly useful when dealing with nominal categorical features—those without any inherent order. It is most effective when the number of categories is manageable, allowing for efficient processing without overwhelming the model with excessive dimensions.\n",
    "\n",
    "In summary, one-hot encoding is a crucial preprocessing step in machine learning that enables categorical data to be effectively utilized in predictive modeling by transforming it into a numerical format that algorithms can interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the Sentence\n",
    "sentence = \"the cat sat on the mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Words: ['on', 'sat', 'cat', 'the', 'mat']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tokenize the Sentence\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = sentence.split()\n",
    "\n",
    "# Get unique words\n",
    "unique_words = list(set(words))\n",
    "\n",
    "# View unique words\n",
    "print(\"Unique Words:\", unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a One-Hot Encoding Function\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def one_hot_encode(sentence):\n",
    "    # Tokenize the sentence\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # Get unique words\n",
    "    unique_words = list(set(words))\n",
    "    \n",
    "    # Create a DataFrame with one-hot encoding\n",
    "    one_hot_df = pd.DataFrame(0, index=range(len(words)), columns=unique_words)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        one_hot_df.at[i, word] = 1\n",
    "    \n",
    "    return one_hot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   on  sat  cat  the  mat\n",
      "0   0    0    0    1    0\n",
      "1   0    0    1    0    0\n",
      "2   0    1    0    0    0\n",
      "3   1    0    0    0    0\n",
      "4   0    0    0    1    0\n",
      "5   0    0    0    0    1\n"
     ]
    }
   ],
   "source": [
    "# Perform one-hot encoding on the sentence\n",
    "one_hot_encoded_df = one_hot_encode(sentence)\n",
    "\n",
    "# View the one-hot encoded DataFrame\n",
    "print(one_hot_encoded_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the Output:\n",
    "\n",
    "- Each row in the DataFrame corresponds to a word in the original sentence.\n",
    "- Each column represents a unique word from the sentence.\n",
    "- A value of 1 indicates that the corresponding word is present in that row (i.e., in that position of the original sentence), while 0 indicates its absence.\n",
    "\n",
    "Summary:\n",
    "\n",
    "This example demonstrates how to perform one-hot encoding on a simple sentence by extracting unique words and representing their presence in a binary format. This technique is useful for preparing text data for machine learning models, allowing algorithms to process categorical text information effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bag of Words (BoW) model is a foundational technique in Natural Language Processing (NLP) and text mining that simplifies the representation of text data for analysis and machine learning. At its core, BoW transforms textual information into a numerical format by treating each document as a collection (or \"bag\") of words, disregarding grammar and word order but preserving the frequency of each word. This approach allows algorithms to process and analyze text data efficiently, making it a popular choice for various applications such as sentiment analysis, document classification, and information retrieval.\n",
    "\n",
    "Key Features of the Bag of Words Model\n",
    "1: Simplicity: The BoW model is straightforward to implement and understand. It requires minimal preprocessing, making it accessible for beginners in NLP.\n",
    "2: Word Frequency Representation: Each document is represented as a vector where each dimension corresponds to a unique word from the entire corpus. The value in each dimension indicates the frequency (or presence) of that word in the document.\n",
    "3: Flexibility: BoW can be adapted to various contexts by adjusting parameters such as vocabulary size, inclusion of n-grams (combinations of words), and handling of stop words (common words that may not contribute significant meaning).\n",
    "\n",
    "Importance in NLP:\n",
    "\n",
    "The Bag of Words model plays a crucial role in transforming unstructured text data into structured numerical data suitable for machine learning algorithms. By converting text into vectors, it enables the application of various statistical and machine learning techniques for tasks such as classification, clustering, and topic modeling. Despite its limitations—such as losing semantic meaning and context due to the disregard for word order—BoW remains a fundamental building block for more advanced NLP techniques like TF-IDF (Term Frequency-Inverse Document Frequency) and word embeddings.\n",
    "\n",
    "In summary, the Bag of Words model is an essential tool in NLP that facilitates the processing and analysis of textual data by converting it into a numerical format. Its simplicity and effectiveness make it a popular choice for many text-based applications, laying the groundwork for more complex language processing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'SpamClassifier-master/smsspamcollection/SMSSpamCollection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m messages\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSpamClassifier-master/smsspamcollection/SMSSpamCollection\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Introduction_AIML/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Introduction_AIML/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Desktop/Introduction_AIML/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Introduction_AIML/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Desktop/Introduction_AIML/venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SpamClassifier-master/smsspamcollection/SMSSpamCollection'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
